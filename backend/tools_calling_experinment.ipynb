{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaiyl/miniconda3/envs/tidbai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from app.core.db import Scoped_Session\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "session = Scoped_Session()\n",
    "turbo = dspy.OpenAI(model='gpt-o1', api_key=os.getenv(\"OPENAI_API_KEY\"), max_tokens=4096)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "user_query = \"Does TiDB provide strict serializability or serializability?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sqlalchemy.exc import SAWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=SAWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.rag.knowledge_graph.graph_store import TiDBGraphStore\n",
    "from app.rag.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "\n",
    "_embed_model = OpenAIEmbedding(\n",
    "    model=OpenAIEmbeddingModelType.TEXT_EMBED_3_SMALL\n",
    ")\n",
    "\n",
    "graph_store = TiDBGraphStore(\n",
    "    dspy_lm=turbo,\n",
    "    session=session,\n",
    "    embed_model=_embed_model,\n",
    ")\n",
    "graph_index =  KnowledgeGraphIndex = KnowledgeGraphIndex.from_existing(\n",
    "    dspy_lm=turbo,\n",
    "    kg_store=graph_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_knowledge_graph(query):\n",
    "    return graph_index.retrieve_with_weight(\n",
    "        query,\n",
    "        [],\n",
    "        depth=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.rag.vector_store.tidb_vector_store import TiDBVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = TiDBVectorStore(session=session)\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=_embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_knowledge_embedded_chunks(query, top_k=5):\n",
    "    retriver = vector_index.as_retriever(\n",
    "        similarity_top_k=5\n",
    "    )\n",
    "\n",
    "    nodes = retriver.retrieve(query)\n",
    "    return [node.text for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import enum\n",
    "import openai\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from sqlmodel import Session\n",
    "import traceback\n",
    "from typing import Generator, Any, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "fc_llm = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "class EventType(str, enum.Enum):\n",
    "    LLM_CONTENT_STREAMING = \"LLM_CONTENT_STREAMING\"\n",
    "    TOOL_CALL = \"TOOL_CALL\"\n",
    "    TOOL_CALL_RESPONSE = \"TOOL_CALL_RESPONSE\"\n",
    "    FINISHED = \"FINISHED\"\n",
    "    ERROR = \"ERROR\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChatEvent:\n",
    "    event_type: EventType\n",
    "    payload: str | dict | None = None\n",
    "\n",
    "    def encode(self, charset) -> bytes:\n",
    "        body = self.payload\n",
    "        body = json.dumps(body, separators=(\",\", \":\"))\n",
    "        return f\"{self.event_type.value}:{body}\\n\".encode(charset)\n",
    "\n",
    "\n",
    "class MessageRole(str, enum.Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: MessageRole\n",
    "    content: str\n",
    "    additional_kwargs: dict[str, Any] = {}\n",
    "\n",
    "\n",
    "# chat api definition\n",
    "class ChatRequest(BaseModel):\n",
    "    user_id: str\n",
    "    messages: list[ChatMessage] = []\n",
    "    metadata: dict = {}\n",
    "    stream: bool = True\n",
    "\n",
    "    @field_validator(\"messages\")\n",
    "    @classmethod\n",
    "    def check_messages(cls, messages: List[ChatMessage]) -> List[ChatMessage]:\n",
    "        if not messages:\n",
    "            raise ValueError(\"messages cannot be empty\")\n",
    "        for m in messages:\n",
    "            if m.role not in [\n",
    "                MessageRole.USER,\n",
    "                MessageRole.ASSISTANT,\n",
    "                MessageRole.TOOL,\n",
    "            ]:\n",
    "                raise ValueError(\"role must be either 'user' or 'assistant'\")\n",
    "            if len(m.content) > 8096:\n",
    "                raise ValueError(\"message content cannot exceed 8096 characters\")\n",
    "        if messages[-1].role != MessageRole.USER:\n",
    "            raise ValueError(\"last message must be from user\")\n",
    "        return messages\n",
    "\n",
    "\n",
    "class GraphKnowledge(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents structured knowledge in the form of a graph, focusing on entities and the relationships between them.\n",
    "\n",
    "    This tool enables users to query and navigate structured relationships between various entities. It is designed to answer questions where understanding the relationships or attributes of specific entities is key to providing an accurate response.\n",
    "\n",
    "    Typical use cases include:\n",
    "    1. **Entity Queries**: Answering questions about the relationships or properties of specific entities.\n",
    "    2. **Relationship Navigation**: Navigating through structured relationships to retrieve specific knowledge tied to entities within the organization's domain.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=(\n",
    "            \"A query for retrieving structured relationships and attributes of specific entities.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "class VectorChunks(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents detailed source data in the form of vectorized chunks, focusing on the content of original documents.\n",
    "\n",
    "    This tool is used to retrieve detailed information based on content similarity. It excels at answering questions requiring deeper context or extended information from original documents, making it suitable for handling more complex or background-intensive queries.\n",
    "\n",
    "    Typical use cases include:\n",
    "    1. **Content Queries**: Providing in-depth answers to questions that require extracting detailed information from original documents.\n",
    "    2. **Context Retrieval**: Handling queries that need extensive background or supporting information by matching content chunks based on similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=(\n",
    "            \"A query for retrieving similar chunks of text that provide detailed context or background information to answer the user's query.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "system_instruction = \"\"\"As the Advanced Query Solver, your primary role is to assist users by breaking down complex queries into manageable subquestions and providing a clear action plan for resolving them. You are responsible for ensuring that each step of the query-solving process is methodically planned and executed, with attention to dependencies between different subquestions.\n",
    "\n",
    "When interacting with users, adhere to the following instructions:\n",
    "\n",
    "1. Query Analysis:\n",
    "   - Upon receiving a user query, begin by analyzing the query to determine its structure and dependencies. \n",
    "   - Prioritize the **Knowledge Graph Search** for querying structured data, which typically provides accurate and concise information about entities and their relationships.\n",
    "   - Create a dependency graph that outlines how each subquestion relates to others and the order in which they should be resolved.\n",
    "      - Example: If the user asks about the status of a project and its potential impact on product delivery, identify the key subquestions: \n",
    "        'What is the current status of the project?'\n",
    "        'Are there any blockers or delays affecting the timeline?'\n",
    "        'What is the estimated impact on the upcoming product release?'\n",
    "      - Ensure that subquestions dependent on the resolution of earlier steps are properly sequenced.\n",
    "\n",
    "2. Subquestion Generation:\n",
    "   - Once the dependency graph is established, break down the original query into a series of subquestions. Each subquestion should be clear, concise, and directly address a portion of the user's original inquiry.\n",
    "   - Ensure that all generated subquestions are relevant to solving the user’s overall query and follow the logical structure of the dependency graph.\n",
    "     - Example: If investigating a system error, break it down into:\n",
    "       'What caused the error?'\n",
    "       'Which systems are affected?'\n",
    "       'What are the potential solutions or workarounds?'\n",
    "   - **Use the Knowledge Graph** whenever possible to answer subquestions involving entities, relationships, or concise information that can be retrieved through structured queries.\n",
    "   - **Search Vector Chunks** when more detailed context or background information is needed, particularly for complex or detailed inquiries that require the retrieval of extensive content.\n",
    "\n",
    "3. Action Plan Generation:\n",
    "   - After breaking down the query into subquestions, generate a clear action plan that specifies how to answer each subquestion and resolve the entire query.\n",
    "   - The action plan should be structured sequentially, reflecting the dependencies outlined in the graph, with clear steps for retrieving or processing the information required to answer each subquestion.\n",
    "     - **Utilize Knowledge Graph** for structured, entity-based information, and **shift to Vector Chunks** when more granular details or in-depth context is needed.\n",
    "     - Example: For a product update query, the action plan might include:\n",
    "       'Step 1: Retrieve the latest project status from the knowledge graph.'\n",
    "       'Step 2: Identify any blockers or delays reported in the last week using vector chunks to gather more details.'\n",
    "       'Step 3: Analyze how these issues might impact the product release timeline.'\n",
    "\n",
    "4. Problem Solving Execution:\n",
    "   - Execute each step of the action plan sequentially, ensuring that all subquestions are answered and that dependencies between them are respected.\n",
    "   - As you solve each subquestion, aggregate the results into a comprehensive response that directly addresses the user's original query.\n",
    "   - Ensure that the final response provides clarity on how each part of the solution was derived and how the user can proceed with the information.\n",
    "\n",
    "5. Communication:\n",
    "   - Clearly explain the steps taken to resolve the user’s query, ensuring they understand the rationale behind each subquestion and how it contributed to the final solution.\n",
    "   - Confirm with the user if additional clarification is needed on any specific subquestion or action plan step.\n",
    "   - Maintain a professional and supportive tone, ensuring that your responses are structured, informative, and actionable for the user’s needs.\n",
    "\n",
    "Your goal is to help users navigate complex queries by decomposing them into logical steps and providing a structured action plan for resolving them. **Prioritize Knowledge Graph Search** for concise, structured answers and **use Vector Chunks** for detailed, context-rich information. Always ensure accuracy, clarity, and logical flow to meet the user’s needs and expectations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ChatService:\n",
    "    def __init__(self):\n",
    "        self.tools = [\n",
    "            openai.pydantic_function_tool(GraphKnowledge),\n",
    "            openai.pydantic_function_tool(VectorChunks),\n",
    "        ]\n",
    "        self._syste_message = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    def chat(\n",
    "        self, session: Session, messages: list = []\n",
    "    ) -> Generator[ChatEvent, None, None]:\n",
    "\n",
    "        if not messages or len(messages) == 0:\n",
    "            yield ChatEvent(event_type=EventType.ERROR, payload=\"No messages provided\")\n",
    "            return\n",
    "\n",
    "        # Step 1: Check if the user exists in the `users` table\n",
    "        response_messages = []\n",
    "\n",
    "        # while condition, if  response.choices[0].message.tool_calls is None, then return; otherwise loop\n",
    "        while True:\n",
    "            response = fc_llm.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=(self._syste_message + messages + response_messages),\n",
    "                tools=self.tools,\n",
    "            )\n",
    "            if response.choices[0].message.tool_calls is None:\n",
    "                yield ChatEvent(\n",
    "                    event_type=EventType.FINISHED,\n",
    "                    payload={\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response.choices[0].message.content,\n",
    "                    },\n",
    "                )\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                tool_call_message = response.choices[0].message.model_dump()\n",
    "                yield ChatEvent(\n",
    "                    event_type=EventType.TOOL_CALL, payload=tool_call_message\n",
    "                )\n",
    "                response_messages.append(tool_call_message)\n",
    "\n",
    "                for tool_call in response.choices[0].message.tool_calls:\n",
    "                    if tool_call.function.name == \"GraphKnowledge\":\n",
    "                        args = json.loads(tool_call.function.arguments)\n",
    "                        print(f\"call function GraphKnowledge\", args)\n",
    "                        graph_data = retrieve_knowledge_graph(args[\"query\"])\n",
    "                        tool_call_result = {\n",
    "                            \"graph_data\": graph_data\n",
    "                        }\n",
    "                    elif tool_call.function.name == \"VectorChunks\":\n",
    "                        args = json.loads(tool_call.function.arguments)\n",
    "                        print(f\"call function VectorChunks\", args)\n",
    "                        graph_data = retrieve_knowledge_graph(args[\"query\"])\n",
    "                        chunks = retrieve_knowledge_embedded_chunks(args[\"query\"])\n",
    "                        tool_call_result = {\n",
    "                            \"chunks_data\": chunks\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"unknow function calling\", tool_call)\n",
    "                        raise ValueError(\n",
    "                            f\"Unknown tool call and message: {response.choices[0].message.model_dump_json()}\"\n",
    "                        )\n",
    "\n",
    "                    tool_call_result_message = {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(tool_call_result),\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                    }\n",
    "\n",
    "                    yield ChatEvent(\n",
    "                        event_type=EventType.TOOL_CALL_RESPONSE,\n",
    "                        payload=tool_call_result_message,\n",
    "                    )\n",
    "\n",
    "                    response_messages.append(tool_call_result_message)\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                session.rollback\n",
    "\n",
    "                yield ChatEvent(\n",
    "                    event_type=EventType.ERROR,\n",
    "                    payload=f\"An error occurred while processing the request.{e}\",\n",
    "                )\n",
    "\n",
    "\n",
    "cs = ChatService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langfuse:Langfuse client is disabled since no public_key was provided as a parameter or environment variable 'LANGFUSE_PUBLIC_KEY'. See our docs: https://langfuse.com/docs/sdk/python/low-level-sdk#initialize-client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {\"role\": \"assistant\", \"content\": \"To address your query effectively, we need to dissect it into manageable subquestions:\\n\\n1. **What is DDL (Data Definition Language)?**\\n   - DDL refers to commands in SQL (Structured Query Language) used to define and modify data structures in a database. \\n\\n2. **What are the roles of DDL in backup processes?**\\n   - Understand how DDL commands influence the creation of database schema that might be backed up.\\n\\n3. **What are the roles of DDL in restore processes?**\\n   - Investigate the influence of DDL commands during the restoration when defining or altering schema structures based on backup data.\\n\\nLet's create a structured approach to address each subquestion.\\n\\n### Action Plan\\n\\n#### Step 1: Explanation of DDL\\n- Provide a basic definition and overview of what DDL encompasses, focusing on its primary commands like `CREATE`, `ALTER`, `DROP`, etc.\\n\\n#### Step 2: Roles in Backup Processes\\n- Investigate how DDL commands are involved in defining data structures prior to data backup.\\n\\n#### Step 3: Roles in Restore Processes\\n- Explore how DDL commands are used during the restore process to recreate or adjust the database structures.\\n\\nLet's start with Step 1 by explaining what DDL is. Once we establish this foundation, we can proceed to steps 2 and 3 to address its roles in backup and restore processes. \\n\\n### Step 1: Explanation of DDL\\n\\nDDL (Data Definition Language) is a subset of SQL commands used to define and manage the structure or schema of a database. DDL commands do not manipulate data directly but are responsible for defining and managing tables, indexes, keys, and other structures. Key DDL statements include:\\n\\n- `CREATE`: Used to create a new table or database.\\n- `ALTER`: Used to modify an existing database object, such as adding or deleting columns in a table.\\n- `DROP`: Used to delete objects from the database.\\n\\nDDL is essential in dictating how data is organized and stored within a database.\\n\\nNow, let's proceed to Step 2 and Step 3 by looking into the roles DDL plays in backup and restore processes. This will be approached using a concise overview since there isn't direct tool interaction here. \\n\\n### Step 2: Roles in Backup Processes\\n\\nIn backup processes, DDL plays a critical role by:\\n\\n- Providing the structure of the database to be backed up, ensuring that the schema can be accurately saved alongside the data.\\n- Commonly, DDL statements such as `CREATE` are executed to define tables, indexes, constraints, and other database objects necessary for a complete backup.\\n\\n### Step 3: Roles in Restore Processes\\n\\nIn restore processes, DDL is crucial because:\\n\\n- The schema defined by DDL during backup must be recreated or adjusted as necessary when restoring from a backup.\\n- DDL commands ensure that the restored database maintains the original structure, allowing data to be correctly inserted back into tables with the appropriate constraints and relationships.\\n\\nBy understanding the definition and roles of DDL in both backup and restore processes, users can better manage database maintenance and integrity. If further, detailed exploration is needed into specific DDL commands or examples, let me know how you'd like to proceed!\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for event in cs.chat(session, [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\": user_query\n",
    "    }]):\n",
    "        print(f\"data: {json.dumps(event.payload)}\\n\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"data: {json.dumps({'event_type': 'ERROR', 'payload': str(e)})}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To address your query about optimizing queries that scan in reverse order of an index, here's a structured plan to tackle the problem. We'll break down the process into clear steps and consider the dependencies involved:\n",
      "\n",
      "### Query Analysis\n",
      "Your query involves understanding how to optimize database queries that require scanning in reverse order when descending indexes are not supported. The key points to focus on are:\n",
      "1. The nature of the database and indexes being used.\n",
      "2. The specific queries that are performing reverse scans.\n",
      "3. The performance impact of these scans.\n",
      "4. Alternative strategies to achieve efficient scans without descending indexes.\n",
      "\n",
      "### Subquestion Generation\n",
      "To solve the problem effectively, we need to answer the following subquestions:\n",
      "1. **What type of database system is being used, and what are its limitations regarding indexes?**\n",
      "2. **Which specific queries are affected by the lack of descending index support?**\n",
      "3. **What current performance metrics can be used to evaluate the impact of reverse scans?**\n",
      "4. **What techniques can be employed to optimize these queries given the database's indexing constraints?**\n",
      "\n",
      "### Action Plan Generation\n",
      "Based on the subquestions outlined above, here is a step-by-step action plan:\n",
      "\n",
      "1. **Identify Database System and Limitations:**\n",
      "   - Use the **Knowledge Graph** to get detailed information about the specific database system, focusing on indexing capabilities and constraints.\n",
      "\n",
      "2. **Analyze Affected Queries:**\n",
      "   - Collect examples of queries that are scanning in reverse order and assess their performance impact.\n",
      "   - Use **Vector Chunks** to retrieve detailed context or background information on similar optimization challenges and solutions for the same or similar database systems.\n",
      "\n",
      "3. **Evaluate Current Performance:**\n",
      "   - Gather performance metrics (e.g., query execution time, resource usage) to quantify the impact of reverse scans.\n",
      "   - Determine any available configuration options or query modifications that might mitigate performance issues.\n",
      "\n",
      "4. **Explore Optimization Techniques:**\n",
      "   - Investigate techniques and strategies to optimize queries, such as:\n",
      "     - Restructuring queries to avoid the need for reverse scans.\n",
      "     - Implementing application-level sorting after retrieving data in the regular indexed order.\n",
      "     - Exploring if compound indexes or covering indexes could improve performance.\n",
      "     - Utilizing database-specific functionality or workarounds to mimic descending index behavior.\n",
      "\n",
      "### Problem Solving Execution\n",
      "Execute each step of the action plan, ensuring dependencies are respected:\n",
      "\n",
      "1. **Identify and Clarify Database Type:**\n",
      "   - Use structured graph-based queries to confirm the database type and its index support details.\n",
      "\n",
      "2. **Gather Knowledge and Context:**\n",
      "   - Use vector retrieval for insights and experiences from similar optimization scenarios.\n",
      "\n",
      "3. **Analyze and Quantify:**\n",
      "   - Measure query performance metrics and consider the potential benefits of different optimization strategies.\n",
      "\n",
      "4. **Implement and Test:**\n",
      "   - Apply feasible optimization techniques and test their effectiveness.\n",
      "\n",
      "### Communication\n",
      "Explain the steps clearly, detailing how each subquestion contributes to the overall query resolution. Confirm with the user if any aspect needs further clarification, focusing on practical and actionable solutions applicable to their specific database environment.\n"
     ]
    }
   ],
   "source": [
    "print(\"To address your query about optimizing queries that scan in reverse order of an index, here's a structured plan to tackle the problem. We'll break down the process into clear steps and consider the dependencies involved:\\n\\n### Query Analysis\\nYour query involves understanding how to optimize database queries that require scanning in reverse order when descending indexes are not supported. The key points to focus on are:\\n1. The nature of the database and indexes being used.\\n2. The specific queries that are performing reverse scans.\\n3. The performance impact of these scans.\\n4. Alternative strategies to achieve efficient scans without descending indexes.\\n\\n### Subquestion Generation\\nTo solve the problem effectively, we need to answer the following subquestions:\\n1. **What type of database system is being used, and what are its limitations regarding indexes?**\\n2. **Which specific queries are affected by the lack of descending index support?**\\n3. **What current performance metrics can be used to evaluate the impact of reverse scans?**\\n4. **What techniques can be employed to optimize these queries given the database's indexing constraints?**\\n\\n### Action Plan Generation\\nBased on the subquestions outlined above, here is a step-by-step action plan:\\n\\n1. **Identify Database System and Limitations:**\\n   - Use the **Knowledge Graph** to get detailed information about the specific database system, focusing on indexing capabilities and constraints.\\n\\n2. **Analyze Affected Queries:**\\n   - Collect examples of queries that are scanning in reverse order and assess their performance impact.\\n   - Use **Vector Chunks** to retrieve detailed context or background information on similar optimization challenges and solutions for the same or similar database systems.\\n\\n3. **Evaluate Current Performance:**\\n   - Gather performance metrics (e.g., query execution time, resource usage) to quantify the impact of reverse scans.\\n   - Determine any available configuration options or query modifications that might mitigate performance issues.\\n\\n4. **Explore Optimization Techniques:**\\n   - Investigate techniques and strategies to optimize queries, such as:\\n     - Restructuring queries to avoid the need for reverse scans.\\n     - Implementing application-level sorting after retrieving data in the regular indexed order.\\n     - Exploring if compound indexes or covering indexes could improve performance.\\n     - Utilizing database-specific functionality or workarounds to mimic descending index behavior.\\n\\n### Problem Solving Execution\\nExecute each step of the action plan, ensuring dependencies are respected:\\n\\n1. **Identify and Clarify Database Type:**\\n   - Use structured graph-based queries to confirm the database type and its index support details.\\n\\n2. **Gather Knowledge and Context:**\\n   - Use vector retrieval for insights and experiences from similar optimization scenarios.\\n\\n3. **Analyze and Quantify:**\\n   - Measure query performance metrics and consider the potential benefits of different optimization strategies.\\n\\n4. **Implement and Test:**\\n   - Apply feasible optimization techniques and test their effectiveness.\\n\\n### Communication\\nExplain the steps clearly, detailing how each subquestion contributes to the overall query resolution. Confirm with the user if any aspect needs further clarification, focusing on practical and actionable solutions applicable to their specific database environment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
