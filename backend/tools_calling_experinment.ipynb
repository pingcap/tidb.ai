{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.core.db import Scoped_Session\n",
    "\n",
    "session = Scoped_Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaiyl/miniconda3/envs/tidbai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "turbo = dspy.OpenAI(model='gpt-o1', api_key=os.getenv(\"OPENAI_API_KEY\"), max_tokens=4096)\n",
    "dspy.settings.configure(lm=turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sqlalchemy.exc import SAWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=SAWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.rag.knowledge_graph.graph_store import TiDBGraphStore\n",
    "from app.rag.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "\n",
    "_embed_model = OpenAIEmbedding(\n",
    "    model=OpenAIEmbeddingModelType.TEXT_EMBED_3_SMALL\n",
    ")\n",
    "\n",
    "graph_store = TiDBGraphStore(\n",
    "    dspy_lm=turbo,\n",
    "    session=session,\n",
    "    embed_model=_embed_model,\n",
    ")\n",
    "graph_index =  KnowledgeGraphIndex = KnowledgeGraphIndex.from_existing(\n",
    "    dspy_lm=turbo,\n",
    "    kg_store=graph_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_knowledge_graph(query):\n",
    "    return graph_index.retrieve_with_weight(\n",
    "        query,\n",
    "        [],\n",
    "        depth=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.rag.vector_store.tidb_vector_store import TiDBVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = TiDBVectorStore(session=session)\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=_embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_knowledge_embedded_chunks(query, top_k=5):\n",
    "    retriver = vector_index.as_retriever(\n",
    "        similarity_top_k=5\n",
    "    )\n",
    "\n",
    "    nodes = retriver.retrieve(query)\n",
    "    return [node.text for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import enum\n",
    "import openai\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from sqlmodel import Session\n",
    "import traceback\n",
    "from typing import Generator, Any, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "fc_llm = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "class EventType(str, enum.Enum):\n",
    "    LLM_CONTENT_STREAMING = \"LLM_CONTENT_STREAMING\"\n",
    "    TOOL_CALL = \"TOOL_CALL\"\n",
    "    TOOL_CALL_RESPONSE = \"TOOL_CALL_RESPONSE\"\n",
    "    FINISHED = \"FINISHED\"\n",
    "    ERROR = \"ERROR\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChatEvent:\n",
    "    event_type: EventType\n",
    "    payload: str | dict | None = None\n",
    "\n",
    "    def encode(self, charset) -> bytes:\n",
    "        body = self.payload\n",
    "        body = json.dumps(body, separators=(\",\", \":\"))\n",
    "        return f\"{self.event_type.value}:{body}\\n\".encode(charset)\n",
    "\n",
    "\n",
    "class MessageRole(str, enum.Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: MessageRole\n",
    "    content: str\n",
    "    additional_kwargs: dict[str, Any] = {}\n",
    "\n",
    "\n",
    "# chat api definition\n",
    "class ChatRequest(BaseModel):\n",
    "    user_id: str\n",
    "    messages: list[ChatMessage] = []\n",
    "    metadata: dict = {}\n",
    "    stream: bool = True\n",
    "\n",
    "    @field_validator(\"messages\")\n",
    "    @classmethod\n",
    "    def check_messages(cls, messages: List[ChatMessage]) -> List[ChatMessage]:\n",
    "        if not messages:\n",
    "            raise ValueError(\"messages cannot be empty\")\n",
    "        for m in messages:\n",
    "            if m.role not in [\n",
    "                MessageRole.USER,\n",
    "                MessageRole.ASSISTANT,\n",
    "                MessageRole.TOOL,\n",
    "            ]:\n",
    "                raise ValueError(\"role must be either 'user' or 'assistant'\")\n",
    "            if len(m.content) > 8096:\n",
    "                raise ValueError(\"message content cannot exceed 8096 characters\")\n",
    "        if messages[-1].role != MessageRole.USER:\n",
    "            raise ValueError(\"last message must be from user\")\n",
    "        return messages\n",
    "\n",
    "\n",
    "class GraphKnowledge(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents structured knowledge in the form of a graph, focusing on entities and the relationships between them.\n",
    "\n",
    "    This tool enables users to query and navigate structured relationships between various entities. It is designed to answer questions where understanding the relationships or attributes of specific entities is key to providing an accurate response.\n",
    "\n",
    "    Typical use cases include:\n",
    "    1. **Entity Queries**: Answering questions about the relationships or properties of specific entities.\n",
    "    2. **Relationship Navigation**: Navigating through structured relationships to retrieve specific knowledge tied to entities within the organization's domain.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=(\n",
    "            \"A query for retrieving structured relationships and attributes of specific entities.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "class VectorChunks(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents detailed source data in the form of vectorized chunks, focusing on the content of original documents.\n",
    "\n",
    "    This tool is used to retrieve detailed information based on content similarity. It excels at answering questions requiring deeper context or extended information from original documents, making it suitable for handling more complex or background-intensive queries.\n",
    "\n",
    "    Typical use cases include:\n",
    "    1. **Content Queries**: Providing in-depth answers to questions that require extracting detailed information from original documents.\n",
    "    2. **Context Retrieval**: Handling queries that need extensive background or supporting information by matching content chunks based on similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=(\n",
    "            \"A query for retrieving similar chunks of text that provide detailed context or background information to answer the user's query.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "system_instruction = \"\"\"As the Advanced Query Solver, your primary role is to assist users by breaking down complex queries into manageable subquestions and providing a clear action plan for resolving them. You are responsible for ensuring that each step of the query-solving process is methodically planned and executed, with attention to dependencies between different subquestions.\n",
    "\n",
    "When interacting with users, adhere to the following instructions:\n",
    "\n",
    "1. Query Analysis:\n",
    "   - Upon receiving a user query, begin by analyzing the query to determine its structure and dependencies. \n",
    "   - Prioritize the **Knowledge Graph Search** for querying structured data, which typically provides accurate and concise information about entities and their relationships.\n",
    "   - Create a dependency graph that outlines how each subquestion relates to others and the order in which they should be resolved.\n",
    "      - Example: If the user asks about the status of a project and its potential impact on product delivery, identify the key subquestions: \n",
    "        'What is the current status of the project?'\n",
    "        'Are there any blockers or delays affecting the timeline?'\n",
    "        'What is the estimated impact on the upcoming product release?'\n",
    "      - Ensure that subquestions dependent on the resolution of earlier steps are properly sequenced.\n",
    "\n",
    "2. Subquestion Generation:\n",
    "   - Once the dependency graph is established, break down the original query into a series of subquestions. Each subquestion should be clear, concise, and directly address a portion of the user's original inquiry.\n",
    "   - Ensure that all generated subquestions are relevant to solving the user’s overall query and follow the logical structure of the dependency graph.\n",
    "     - Example: If investigating a system error, break it down into:\n",
    "       'What caused the error?'\n",
    "       'Which systems are affected?'\n",
    "       'What are the potential solutions or workarounds?'\n",
    "   - **Use the Knowledge Graph** whenever possible to answer subquestions involving entities, relationships, or concise information that can be retrieved through structured queries.\n",
    "   - **Search Vector Chunks** when more detailed context or background information is needed, particularly for complex or detailed inquiries that require the retrieval of extensive content.\n",
    "\n",
    "3. Action Plan Generation:\n",
    "   - After breaking down the query into subquestions, generate a clear action plan that specifies how to answer each subquestion and resolve the entire query.\n",
    "   - The action plan should be structured sequentially, reflecting the dependencies outlined in the graph, with clear steps for retrieving or processing the information required to answer each subquestion.\n",
    "     - **Utilize Knowledge Graph** for structured, entity-based information, and **shift to Vector Chunks** when more granular details or in-depth context is needed.\n",
    "     - Example: For a product update query, the action plan might include:\n",
    "       'Step 1: Retrieve the latest project status from the knowledge graph.'\n",
    "       'Step 2: Identify any blockers or delays reported in the last week using vector chunks to gather more details.'\n",
    "       'Step 3: Analyze how these issues might impact the product release timeline.'\n",
    "\n",
    "4. Problem Solving Execution:\n",
    "   - Execute each step of the action plan sequentially, ensuring that all subquestions are answered and that dependencies between them are respected.\n",
    "   - As you solve each subquestion, aggregate the results into a comprehensive response that directly addresses the user's original query.\n",
    "   - Ensure that the final response provides clarity on how each part of the solution was derived and how the user can proceed with the information.\n",
    "\n",
    "5. Communication:\n",
    "   - Clearly explain the steps taken to resolve the user’s query, ensuring they understand the rationale behind each subquestion and how it contributed to the final solution.\n",
    "   - Confirm with the user if additional clarification is needed on any specific subquestion or action plan step.\n",
    "   - Maintain a professional and supportive tone, ensuring that your responses are structured, informative, and actionable for the user’s needs.\n",
    "\n",
    "Your goal is to help users navigate complex queries by decomposing them into logical steps and providing a structured action plan for resolving them. **Prioritize Knowledge Graph Search** for concise, structured answers and **use Vector Chunks** for detailed, context-rich information. Always ensure accuracy, clarity, and logical flow to meet the user’s needs and expectations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ChatService:\n",
    "    def __init__(self):\n",
    "        self.tools = [\n",
    "            openai.pydantic_function_tool(GraphKnowledge),\n",
    "            openai.pydantic_function_tool(VectorChunks),\n",
    "        ]\n",
    "        self._syste_message = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    def chat(\n",
    "        self, session: Session, messages: list = []\n",
    "    ) -> Generator[ChatEvent, None, None]:\n",
    "\n",
    "        if not messages or len(messages) == 0:\n",
    "            yield ChatEvent(event_type=EventType.ERROR, payload=\"No messages provided\")\n",
    "            return\n",
    "\n",
    "        # Step 1: Check if the user exists in the `users` table\n",
    "        response_messages = []\n",
    "\n",
    "        # while condition, if  response.choices[0].message.tool_calls is None, then return; otherwise loop\n",
    "        while True:\n",
    "            response = fc_llm.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=(self._syste_message + messages + response_messages),\n",
    "                tools=self.tools,\n",
    "            )\n",
    "            if response.choices[0].message.tool_calls is None:\n",
    "                yield ChatEvent(\n",
    "                    event_type=EventType.FINISHED,\n",
    "                    payload={\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response.choices[0].message.content,\n",
    "                    },\n",
    "                )\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                tool_call_message = response.choices[0].message.model_dump()\n",
    "                yield ChatEvent(\n",
    "                    event_type=EventType.TOOL_CALL, payload=tool_call_message\n",
    "                )\n",
    "                response_messages.append(tool_call_message)\n",
    "\n",
    "                for tool_call in response.choices[0].message.tool_calls:\n",
    "                    if tool_call.function.name == \"GraphKnowledge\":\n",
    "                        args = json.loads(tool_call.function.arguments)\n",
    "                        print(f\"call function GraphKnowledge\", args)\n",
    "                        graph_data = retrieve_knowledge_graph(args[\"query\"])\n",
    "                        tool_call_result = {\n",
    "                            \"graph_data\": graph_data\n",
    "                        }\n",
    "                    elif tool_call.function.name == \"VectorChunks\":\n",
    "                        args = json.loads(tool_call.function.arguments)\n",
    "                        print(f\"call function VectorChunks\", args)\n",
    "                        graph_data = retrieve_knowledge_graph(args[\"query\"])\n",
    "                        chunks = retrieve_knowledge_embedded_chunks(args[\"query\"])\n",
    "                        tool_call_result = {\n",
    "                            \"chunks_data\": chunks\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"unknow function calling\", tool_call)\n",
    "                        raise ValueError(\n",
    "                            f\"Unknown tool call and message: {response.choices[0].message.model_dump_json()}\"\n",
    "                        )\n",
    "\n",
    "                    tool_call_result_message = {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(tool_call_result),\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                    }\n",
    "\n",
    "                    yield ChatEvent(\n",
    "                        event_type=EventType.TOOL_CALL_RESPONSE,\n",
    "                        payload=tool_call_result_message,\n",
    "                    )\n",
    "\n",
    "                    response_messages.append(tool_call_result_message)\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                session.rollback\n",
    "\n",
    "                yield ChatEvent(\n",
    "                    event_type=EventType.ERROR,\n",
    "                    payload=f\"An error occurred while processing the request.{e}\",\n",
    "                )\n",
    "\n",
    "\n",
    "cs = ChatService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {\"content\": \"To provide a comprehensive summary of the performance improvements in TiDB from version 6.5 to the newest version, we need to break this query into several manageable subquestions to thoroughly capture and analyze the data:\\n\\n1. **Identify the Versions**: Determine the newest version of TiDB so we can define the scope of the comparison.\\n2. **Review Release Notes**: Extract performance-related improvements, bug fixes, and feature enhancements from the release notes of the versions between 6.5 and the newest version.\\n3. **Categorize Improvements**: Classify the improvements into different categories such as query performance, storage optimization, system stability, new features, etc.\\n4. **Compare Performance Metrics**: If available, compare quantitative performance metrics (e.g., query execution time, transaction per second) between version 6.5 and the newest version.\\n5. **Summarize Findings**: Consolidate the extracted data into a coherent summary that highlights the key performance improvements.\\n\\n### Action Plan\\n1. **Determine the Newest Version of TiDB**:\\n   - Query the structured knowledge graph to find the newest version of TiDB after version 6.5.\\n\\n2. **Extract Release Notes for Each Relevant Version**:\\n   - Fetch detailed release notes and performance metrics for versions from 6.5 to the newest version using vector chunk searches.\\n\\n3. **Analyze and Categorize Extracted Data**:\\n   - Review the content of the release notes to identify and categorize performance improvements.\\n\\n4. **Compare Quantitative Metrics**:\\n   - Identify and compare any available performance metrics data from version 6.5 and the newest version.\\n\\n5. **Draft a Performance Improvement Summary**:\\n   - Summarize the findings into key performance improvements from version 6.5 to the newest version.\\n\\n### Step 1: Determine the Newest Version of TiDB\\nI'll start with identifying the latest version of TiDB using the knowledge graph.\", \"refusal\": null, \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": [{\"id\": \"call_Redcqjga9jG6ODDrmKTYAGHA\", \"function\": {\"arguments\": \"{\\\"query\\\":\\\"What is the newest version of TiDB?\\\"}\", \"name\": \"GraphKnowledge\"}, \"type\": \"function\"}]}\n",
      "\n",
      "\n",
      "call function GraphKnowledge {'query': 'What is the newest version of TiDB?'}\n",
      "data: {\"role\": \"tool\", \"content\": \"{\\\"graph_data\\\": [[{\\\"id\\\": 63811, \\\"name\\\": \\\"v6.3.0\\\", \\\"description\\\": \\\"v6.3.0 is a version of TiDB.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 32838, \\\"name\\\": \\\"latest stable version\\\", \\\"description\\\": \\\"The latest stable version is the most recent release of a component that has been thoroughly tested and is considered production-ready. It is the most recent release that is considered stable and ready for production use. \\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 13263, \\\"name\\\": \\\"TiDB 5.4.3\\\", \\\"description\\\": \\\"TiDB 5.4.3 is a specific version of the TiDB database, released on October 13, 2022.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 56720, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is a distributed SQL database that is designed to be scalable, reliable, and easy to use. It is also designed to be highly scalable and available. Secondary indexes, foreign keys, and user accounts must be manually created in TiDB.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 59284, \\\"name\\\": \\\"v7.1.1\\\", \\\"description\\\": \\\"A specific version of TiDB.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 52117, \\\"name\\\": \\\"TiDB 2.1.8\\\", \\\"description\\\": \\\"TiDB 2.1.8 is a specific version of the TiDB database released on April 12, 2019.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 15831, \\\"name\\\": \\\"TiDB Version\\\", \\\"description\\\": \\\"A TiDB version represents a specific release of the TiDB software, with each version containing new features, bug fixes, and performance improvements. It is identified by a unique version number and indicates the features and capabilities of the TiDB database system. \\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 11738, \\\"name\\\": \\\"TiDB 2.1\\\", \\\"description\\\": \\\"TiDB 2.1 is a previous version of the TiDB database. It is a specific version of TiDB.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 132890, \\\"name\\\": \\\"Vector Search Limitation\\\", \\\"description\\\": \\\"Outline several limitations and restrictions associated with TiDB Vector. Here are the key limitations:\\\\n\\\\n1. Availability: TiDB Vector Search is currently only available for TiDB Serverless clusters. It is will be available for TiDB Dedicated or TiDB Self-Hosted deployments[^1].\\\\n\\\\n2. Vector Dimensions: Each vector can have a maximum of 16,000 dimensions[^1][^2].\\\\n\\\\n3. Data Type: Vector data must be single-precision floating-point numbers (Float32). Double-precision floating numbers are currently not supported for Vector data types, although this feature is planned for a future release[^2].\\\\n\\\\n4. Unsupported Values: You cannot store `NaN`, `Infinity`, or `-Infinity` values in the vector data type[^2].\\\\n\\\\n5. Index Creation: Vector indexes can only be defined and created when the table is created. You cannot create the vector index on demand using DDL statements after the table is created. Similarly, you cannot drop the vector index using DDL statements[^3]. \\\\n\\\\n6. Distance Metrics: Only cosine distance and L2 distance are supported for creating vector search indexes. L1 distance and inner product are not currently supported for vector indexes[^3].\\\\n\\\\nIt's important to note that TiDB is continuously working to enhance the Vector Search feature, and some of these limitations may be addressed in future releases.\\\\n\\\\nExample SQL:\\\\n\\\\n- To create an HNSW vector index, specify the index definition in the comment of a column with a vector data type when creating the table. For simplicity, we will use a 3-dimensional vector.\\\\n```sql\\\\nCREATE TABLE vector_table_with_index (id INT PRIMARY KEY, embedding VECTOR(3) COMMENT 'hnsw(distance=cosine)');\\\\n```\\\\n\\\\n- Insert some sample data into the table.\\\\n```\\\\nINSERT INTO vector_table_with_index (id, embedding) VALUES (1, '[1.0, 2.0, 3.0]'), (2, '[4.0, 5.0, 6.0]'), (3, '[7.0, 8.0, 9.0]');\\\\n```\\\\n\\\\n- Perform a vector search to find the two closest vectors to '[1.0, 2.0, 3.0]' using cosine distance.\\\\n```sql\\\\nSELECT id, embedding, VEC_COSINE_DISTANCE(embedding, '[1.0, 2.0, 3.0]') AS distance FROM vector_table_with_index ORDER BY distance LIMIT 2;\\\\n```\\\\n\\\\n\\\\n[^1]: [Vector Search Limitations | PingCAP Docs](https://docs.pingcap.com/tidbcloud/vector-search-limitations)\\\\n[^2]: [Vector Search Data Types | PingCAP Docs](https://docs.pingcap.com/tidbcloud/vector-search-data-types)\\\\n[^3]: [Vector Search Index | PingCAP Docs](https://docs.pingcap.com/tidbcloud/vector-search-index)\\\\n[^4]: [Continuous Profiling | PingCAP Docs](https://docs.pingcap.com/tidb/v5.3/continuous-profiling)\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"synopsis\\\"}, {\\\"id\\\": 51257, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is an open source distributed HTAP database compatible with the MySQL protocol. TiDB is a distributed SQL database that is compatible with MySQL.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 608, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is a distributed relational database management system (RDBMS) that is compatible with MySQL. TiDB is a distributed SQL database that is compatible with MySQL. It is designed to be highly scalable and available. Deployment is simplified by TiDB Operator. TiDB is a MySQL-compatible database, it is a distributed relational database management system (RDBMS) that is designed to be scalable, highly available, and consistent.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 15650, \\\"name\\\": \\\"Release Date\\\", \\\"description\\\": \\\"The date on which a new version of a software product is made available to the public.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 1698, \\\"name\\\": \\\"TiDB version\\\", \\\"description\\\": \\\"A specific release of the TiDB database software, identified by a version number. It may include new features, bug fixes, or performance improvements. A specific release of the TiDB database system, identified by a version number. TiDB version is a specific version of the TiDB database software. TiDB Cloud will regularly upgrade the TiDB version that is too low uniformly.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 2918, \\\"name\\\": \\\"v2.0.7\\\", \\\"description\\\": \\\"Derived from from relationship: TiDB -> The TiDB version is bumped to v2.0.7. -> v2.0.7\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 14058, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is an open-source distributed relational database management system (RDBMS) that is highly compatible with the MySQL protocol. TiDB is a distributed SQL database that is compatible with MySQL. NULL values are sorted by the `ORDER BY` clause in the following cases:\\\\n- In the `ORDER BY column ASC` statement, `NULL` values are returned first.\\\\n- In the `ORDER BY column DESC` statement, `NULL` values are returned last.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 102890, \\\"name\\\": \\\"TiDB Server\\\", \\\"description\\\": \\\"TiDB Server is a core component of the TiDB distributed database system. It acts as the SQL layer, processing and executing SQL queries, and serves as the interface between the end-users\\\\u2019 applications and the distributed storage layer of TiDB. TiDB Server is designed to be MySQL-compatible, allowing for seamless integration with existing MySQL applications. It efficiently handles OLTP (Online Transactional Processing) workloads, making it suitable for large-scale, high-concurrency environments.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"synopsis\\\"}, {\\\"id\\\": 64172, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is a distributed SQL database that is compatible with MySQL and offers high availability, scalability, and performance. TiDB aims to provide MySQL compatibility while offering scalability and high availability. It supports features like optimizer hints, runaway query management, and partitioned tables. TiDB is designed to be highly scalable and available, and it can handle large amounts of data.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 8880, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is a distributed SQL database that is designed to be highly scalable, available, and reliable. TiDB is a distributed SQL database that is designed to be highly scalable and available. It provides high availability, scalability, and performance. TiDB is used for data computing. TiDB is also designed to be scalable, reliable, and easy to use. TiDB cluster remains available even when one zone fails. TiDB is a distributed SQL database that is designed to be scalable, reliable, and easy to use. TiDB is a distributed SQL database that is designed to be scalable, reliable, and highly available.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 41142, \\\"name\\\": \\\"TiDB 3.0\\\", \\\"description\\\": \\\"TiDB 3.0 is a version of the TiDB database that was released on June 28, 2019. TiDB 3.0 introduced new features and improvements.  DISTRIBUTED GC mode became the default in this version.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 10425, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is a distributed SQL database that is compatible with MySQL. It is designed to handle large datasets and high-volume transactions, making it suitable for demanding applications. TiDB is a distributed SQL database that is compatible with MySQL and offers high availability, scalability, and performance. It is designed for high availability, scalability, and performance. TiDB is a distributed SQL database that is designed to handle large datasets and high-volume transactions, making it suitable for demanding applications. TiDB is a distributed, scalable, and transactional database that is designed for handling large volumes of data. It is an open-source database that is compatible with MySQL and provides high performance, availability, and scalability. TiDB is an extension to MySQL syntax.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}, {\\\"id\\\": 41022, \\\"name\\\": \\\"TiDB\\\", \\\"description\\\": \\\"TiDB is a distributed SQL database system that is part of the TiDB ecosystem. It is a distributed SQL database, and v6.6.0 introduced Partitioned Raft KV.\\\", \\\"meta\\\": null, \\\"entity_type\\\": \\\"original\\\"}], [{\\\"id\\\": 19870, \\\"source_entity_id\\\": 14058, \\\"target_entity_id\\\": 15831, \\\"description\\\": \\\"TiDB 5.1 Release Notes, TiDB version: 5.1.0\\\", \\\"rag_description\\\": \\\"TiDB -> TiDB 5.1 Release Notes, TiDB version: 5.1.0 -> TiDB Version\\\", \\\"weight\\\": 0}, {\\\"id\\\": 3333, \\\"source_entity_id\\\": 608, \\\"target_entity_id\\\": 2918, \\\"description\\\": \\\"The TiDB version is bumped to v2.0.7.\\\", \\\"rag_description\\\": \\\"TiDB -> The TiDB version is bumped to v2.0.7. -> v2.0.7\\\", \\\"weight\\\": 0}, {\\\"id\\\": 65798, \\\"source_entity_id\\\": 41142, \\\"target_entity_id\\\": 11738, \\\"description\\\": \\\"TiDB 3.0 is a newer version of the TiDB database compared to TiDB 2.1.\\\", \\\"rag_description\\\": \\\"TiDB 3.0 -> TiDB 3.0 is a newer version of the TiDB database compared to TiDB 2.1. -> TiDB 2.1\\\", \\\"weight\\\": 0}, {\\\"id\\\": 19814, \\\"source_entity_id\\\": 14058, \\\"target_entity_id\\\": 15650, \\\"description\\\": \\\"TiDB 5.1 Release Notes, Release date: June 24, 2021\\\", \\\"rag_description\\\": \\\"TiDB -> TiDB 5.1 Release Notes, Release date: June 24, 2021 -> Release Date\\\", \\\"weight\\\": 0}, {\\\"id\\\": 73050, \\\"source_entity_id\\\": 52117, \\\"target_entity_id\\\": 51257, \\\"description\\\": \\\"TiDB 2.1.8 is a specific version of the TiDB database.\\\", \\\"rag_description\\\": \\\"TiDB 2.1.8 -> TiDB 2.1.8 is a specific version of the TiDB database. -> TiDB\\\", \\\"weight\\\": 0}, {\\\"id\\\": 17005, \\\"source_entity_id\\\": 13263, \\\"target_entity_id\\\": 8880, \\\"description\\\": \\\"TiDB 5.4.3 is a specific version of the TiDB database.\\\", \\\"rag_description\\\": \\\"TiDB 5.4.3 -> TiDB 5.4.3 is a specific version of the TiDB database. -> TiDB\\\", \\\"weight\\\": 0}, {\\\"id\\\": 57460, \\\"source_entity_id\\\": 41022, \\\"target_entity_id\\\": 32838, \\\"description\\\": \\\"TiDB has a latest stable version that can be installed using TiUP.\\\", \\\"rag_description\\\": \\\"TiDB -> TiDB has a latest stable version that can be installed using TiUP. -> latest stable version\\\", \\\"weight\\\": 0}, {\\\"id\\\": 104346, \\\"source_entity_id\\\": 64172, \\\"target_entity_id\\\": 63811, \\\"description\\\": \\\"TiDB version v6.3.0 is the new default version for new Developer Tier clusters in TiDB Cloud.\\\", \\\"rag_description\\\": \\\"TiDB -> TiDB version v6.3.0 is the new default version for new Developer Tier clusters in TiDB Cloud. -> v6.3.0\\\", \\\"weight\\\": 0}, {\\\"id\\\": 17054, \\\"source_entity_id\\\": 10425, \\\"target_entity_id\\\": 1698, \\\"description\\\": \\\"TiDB has different versions, each identified by a version number.\\\", \\\"rag_description\\\": \\\"TiDB -> TiDB has different versions, each identified by a version number. -> TiDB version\\\", \\\"weight\\\": 0}, {\\\"id\\\": 88767, \\\"source_entity_id\\\": 56720, \\\"target_entity_id\\\": 59284, \\\"description\\\": \\\"v7.1.1 is a specific version of TiDB.\\\", \\\"rag_description\\\": \\\"TiDB -> v7.1.1 is a specific version of TiDB. -> v7.1.1\\\", \\\"weight\\\": 0}], []]}\", \"tool_call_id\": \"call_Redcqjga9jG6ODDrmKTYAGHA\"}\n",
      "\n",
      "\n",
      "data: {\"content\": \"The newest stable version of TiDB is **v7.1.1**.\\n\\n### Step 2: Extract Release Notes for Each Relevant Version\\nWe will now fetch detailed release notes and performance metrics for versions from 6.5 up to 7.1.1. To do this, we need to perform vector chunk searches for each of these versions.\\n\\n### Action Plan\\n1. **Fetch Release Notes for v6.6.0, v7.0.0, and v7.1.1**: Extract performance-related improvements and changes from the release notes of these versions.\\n\\nLet's proceed with the vector chunk searches for comprehensive release notes for these versions.\", \"refusal\": null, \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": [{\"id\": \"call_3LvLEfjdpcB1uV2ftxRI1CR0\", \"function\": {\"arguments\": \"{\\\"query\\\": \\\"TiDB v6.6.0 release notes performance improvements\\\"}\", \"name\": \"VectorChunks\"}, \"type\": \"function\"}, {\"id\": \"call_DeiHtFoMG5kpIPx4cDoVdR3r\", \"function\": {\"arguments\": \"{\\\"query\\\": \\\"TiDB v7.0.0 release notes performance improvements\\\"}\", \"name\": \"VectorChunks\"}, \"type\": \"function\"}, {\"id\": \"call_3HfU3Wef0cPmM29fZDIBowhp\", \"function\": {\"arguments\": \"{\\\"query\\\": \\\"TiDB v7.1.1 release notes performance improvements\\\"}\", \"name\": \"VectorChunks\"}, \"type\": \"function\"}]}\n",
      "\n",
      "\n",
      "call function VectorChunks {'query': 'TiDB v6.6.0 release notes performance improvements'}\n",
      "data: {\"role\": \"tool\", \"content\": \"{\\\"chunks_data\\\": [\\\"---\\\\ntitle: TiDB 6.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 6.5.0.\\\\n---\\\\n\\\\n# TiDB 6.5.0 Release Notes\\\\n\\\\nRelease date: December 29, 2022\\\\n\\\\nTiDB version: 6.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v6.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v6.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 6.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with TiDB [6.4.0-DMR](/releases/release-6.4.0.md), TiDB 6.5.0 introduces the following key features and improvements:\\\\n\\\\n> **Tip:**\\\\n>\\\\n> Compared with the previous LTS 6.1.0, TiDB 6.5.0 also includes new features, improvements, and bug fixes released in [6.2.0-DMR](/releases/release-6.2.0.md), [6.3.0-DMR](/releases/release-6.3.0.md), and [6.4.0-DMR](/releases/release-6.4.0.md).\\\\n>\\\\n> - To get a full list of changes between the 6.1.0 LTS and 6.5.0 LTS versions, in addition to this release notes, also see [6.2.0-DMR release notes](/releases/release-6.2.0.md), [6.3.0-DMR release notes](/releases/release-6.3.0.md), and [6.4.0-DMR release notes](/releases/release-6.4.0.md).\\\\n> - To have a quick comparison of key features between the 6.1.0 LTS and 6.5.0 LTS versions, you can check the `v6.1` and `v6.5` columns in [TiDB features](/basic-features.md).\\\\n\\\\n- The [index acceleration](/system-variables.md#tidb_ddl_enable_fast_reorg-new-in-v630) feature becomes generally available (GA), which improves the performance of adding indexes by about 10 times compared with v6.1.0.\\\\n- The TiDB global memory control becomes GA, and you can control the memory consumption threshold via [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-new-in-v640).\\\\n- The high-performance and globally monotonic [`AUTO_INCREMENT`](/auto-increment.md#mysql-compatibility-mode) column attribute becomes GA, which is compatible with MySQL.\\\\n- [`FLASHBACK CLUSTER TO TIMESTAMP`](/sql-statements/sql-statement-flashback-cluster.md) is now compatible with TiCDC and PITR and becomes GA.\\\\n- Enhance TiDB optimizer by making the more accurate [Cost Model version 2](/cost-model.md#cost-model-version-2) generally available and supporting expressions connected by `AND` for [INDEX MERGE](/explain-index-merge.md).\\\\n- Support pushing down the `JSON_EXTRACT()` function to TiFlash.\\\\n- Support [password management](/password-management.md) policies that meet password compliance auditing requirements.\\\\n- TiDB Lightning and Dumpling support [importing](/tidb-lightning/tidb-lightning-data-source.md) and [exporting](/dumpling-overview.md#improve-export-efficiency-through-concurrency) compressed SQL and CSV files.\\\\n- TiDB Data Migration (DM) [continuous data validation](/dm/dm-continuous-data-validation.md) becomes GA.\\\\n- TiDB Backup & Restore supports snapshot checkpoint backup, improves the recovery performance of [PITR](/br/br-pitr-guide.md#run-pitr) by 50%, and reduces the RPO in common scenarios to as short as 5 minutes.\\\\n- Improve the TiCDC throughput of [replicating data to Kafka](/replicate-data-to-kafka.md) from 4000 rows/s to 35000 rows/s, and reduce the replication latency to 2s.\\\\n- Provide row-level [Time to live (TTL)](/time-to-live.md) to manage data lifecycle (experimental).\\\\n- TiCDC supports [replicating changed logs to object storage](/ticdc/ticdc-sink-to-cloud-storage.md) such as Amazon S3, Azure Blob Storage, and NFS (experimental).\\\", \\\"---\\\\ntitle: TiDB 6.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 6.5.0.\\\\n---\\\\n\\\\n# TiDB 6.5.0 Release Notes\\\\n\\\\nRelease date: December 29, 2022\\\\n\\\\nTiDB version: 6.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v6.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v6.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 6.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with TiDB [6.4.0-DMR](/releases/release-6.4.0.md), TiDB 6.5.0 introduces the following key features and improvements:\\\\n\\\\n> **Tip:**\\\\n>\\\\n> Compared with the previous LTS 6.1.0, TiDB 6.5.0 also includes new features, improvements, and bug fixes released in [6.2.0-DMR](/releases/release-6.2.0.md), [6.3.0-DMR](/releases/release-6.3.0.md), and [6.4.0-DMR](/releases/release-6.4.0.md).\\\\n>\\\\n> - To get a full list of changes between the 6.1.0 LTS and 6.5.0 LTS versions, in addition to this release notes, also see [6.2.0-DMR release notes](/releases/release-6.2.0.md), [6.3.0-DMR release notes](/releases/release-6.3.0.md), and [6.4.0-DMR release notes](/releases/release-6.4.0.md).\\\\n> - To have a quick comparison of key features between the 6.1.0 LTS and 6.5.0 LTS versions, you can check the `v6.1` and `v6.5` columns in [TiDB features](/basic-features.md).\\\\n\\\\n- The [index acceleration](/system-variables.md#tidb_ddl_enable_fast_reorg-new-in-v630) feature becomes generally available (GA), which improves the performance of adding indexes by about 10 times compared with v6.1.0.\\\\n- The TiDB global memory control becomes GA, and you can control the memory consumption threshold via [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-new-in-v640).\\\\n- The high-performance and globally monotonic [`AUTO_INCREMENT`](/auto-increment.md#mysql-compatibility-mode) column attribute becomes GA, which is compatible with MySQL.\\\\n- [`FLASHBACK CLUSTER TO TIMESTAMP`](/sql-statements/sql-statement-flashback-cluster.md) is now compatible with TiCDC and PITR and becomes GA.\\\\n- Enhance TiDB optimizer by making the more accurate [Cost Model version 2](/cost-model.md#cost-model-version-2) generally available and supporting expressions connected by `AND` for [INDEX MERGE](/explain-index-merge.md).\\\\n- Support pushing down the `JSON_EXTRACT()` function to TiFlash.\\\\n- Support [password management](/password-management.md) policies that meet password compliance auditing requirements.\\\\n- TiDB Lightning and Dumpling support [importing](/tidb-lightning/tidb-lightning-data-source.md) and [exporting](/dumpling-overview.md#improve-export-efficiency-through-concurrency) compressed SQL and CSV files.\\\\n- TiDB Data Migration (DM) [continuous data validation](/dm/dm-continuous-data-validation.md) becomes GA.\\\\n- TiDB Backup & Restore supports snapshot checkpoint backup, improves the recovery performance of [PITR](/br/br-pitr-guide.md#run-pitr) by 50%, and reduces the RPO in common scenarios to as short as 5 minutes.\\\\n- Improve the TiCDC throughput of [replicating data to Kafka](/replicate-data-to-kafka.md) from 4000 rows/s to 35000 rows/s, and reduce the replication latency to 2s.\\\\n- Provide row-level [Time to live (TTL)](/time-to-live.md) to manage data lifecycle (experimental).\\\\n- TiCDC supports [replicating changed logs to object storage](/ticdc/ticdc-sink-to-cloud-storage.md) such as Amazon S3, Azure Blob Storage, and NFS (experimental).\\\", \\\"---\\\\ntitle: TiDB 6.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 6.5.0.\\\\n---\\\\n\\\\n# TiDB 6.5.0 Release Notes\\\\n\\\\n<EmailSubscriptionWrapper />\\\\n\\\\nRelease date: December 29, 2022\\\\n\\\\nTiDB version: 6.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v6.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v6.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 6.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with TiDB [6.4.0-DMR](/releases/release-6.4.0.md), TiDB 6.5.0 introduces the following key features and improvements:\\\\n\\\\n> **Tip:**\\\\n>\\\\n> Compared with the previous LTS 6.1.0, TiDB 6.5.0 also includes new features, improvements, and bug fixes released in [6.2.0-DMR](/releases/release-6.2.0.md), [6.3.0-DMR](/releases/release-6.3.0.md), and [6.4.0-DMR](/releases/release-6.4.0.md).\\\\n>\\\\n> - To get a full list of changes between the 6.1.0 LTS and 6.5.0 LTS versions, in addition to this release notes, also see [6.2.0-DMR release notes](/releases/release-6.2.0.md), [6.3.0-DMR release notes](/releases/release-6.3.0.md), and [6.4.0-DMR release notes](/releases/release-6.4.0.md).\\\\n> - To have a quick comparison of key features between the 6.1.0 LTS and 6.5.0 LTS versions, you can check the `v6.1` and `v6.5` columns in [TiDB features](/basic-features.md).\\\\n\\\\n- The [index acceleration](/system-variables.md#tidb_ddl_enable_fast_reorg-new-in-v630) feature becomes generally available (GA), which improves the performance of adding indexes by about 10 times compared with v6.1.0.\\\\n- The TiDB global memory control becomes GA, and you can control the memory consumption threshold via [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-new-in-v640).\\\\n- The high-performance and globally monotonic [`AUTO_INCREMENT`](/auto-increment.md#mysql-compatibility-mode) column attribute becomes GA, which is compatible with MySQL.\\\\n- [`FLASHBACK CLUSTER TO TIMESTAMP`](/sql-statements/sql-statement-flashback-cluster.md) is now compatible with TiCDC and PITR and becomes GA.\\\\n- Enhance TiDB optimizer by making the more accurate [Cost Model version 2](/cost-model.md#cost-model-version-2) generally available and supporting expressions connected by `AND` for [INDEX MERGE](/explain-index-merge.md).\\\\n- Support pushing down the `JSON_EXTRACT()` function to TiFlash.\\\\n- Support [password management](/password-management.md) policies that meet password compliance auditing requirements.\\\\n- TiDB Lightning and Dumpling support [importing](/tidb-lightning/tidb-lightning-data-source.md) and [exporting](/dumpling-overview.md#improve-export-efficiency-through-concurrency) compressed SQL and CSV files.\\\\n- TiDB Data Migration (DM) [continuous data validation](/dm/dm-continuous-data-validation.md) becomes GA.\\\\n- TiDB Backup & Restore supports snapshot checkpoint backup, improves the recovery performance of [PITR](/br/br-pitr-guide.md#run-pitr) by 50%, and reduces the RPO in common scenarios to as short as 5 minutes.\\\\n- Improve the TiCDC throughput of [replicating data to Kafka](/replicate-data-to-kafka.md) from 4000 rows/s to 35000 rows/s, and reduce the replication latency to 2s.\\\\n- Provide row-level [Time to live (TTL)](/time-to-live.md) to manage data lifecycle (experimental).\\\\n- TiCDC supports [replicating changed logs to object storage](/ticdc/ticdc-sink-to-cloud-storage.md) such as Amazon S3, Azure Blob Storage, and NFS (experimental).\\\", \\\"---\\\\ntitle: TiDB 6.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 6.5.0.\\\\n---\\\\n\\\\n# TiDB 6.5.0 Release Notes\\\\n\\\\nRelease date: December 29, 2022\\\\n\\\\nTiDB version: 6.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v6.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v6.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 6.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with TiDB [6.4.0-DMR](/releases/release-6.4.0.md), TiDB 6.5.0 introduces the following key features and improvements:\\\\n\\\\n> **Tip:**\\\\n>\\\\n> Compared with the previous LTS 6.1.0, TiDB 6.5.0 also includes new features, improvements, and bug fixes released in [6.2.0-DMR](/releases/release-6.2.0.md), [6.3.0-DMR](/releases/release-6.3.0.md), and [6.4.0-DMR](/releases/release-6.4.0.md).\\\\n>\\\\n> - To get a full list of changes between the 6.1.0 LTS and 6.5.0 LTS versions, in addition to this release notes, also see [6.2.0-DMR release notes](/releases/release-6.2.0.md), [6.3.0-DMR release notes](/releases/release-6.3.0.md), and [6.4.0-DMR release notes](/releases/release-6.4.0.md).\\\\n> - To have a quick comparison of key features between the 6.1.0 LTS and 6.5.0 LTS versions, you can check the `v6.1` and `v6.5` columns in [TiDB features](/basic-features.md).\\\\n\\\\n- The [index acceleration](/system-variables.md#tidb_ddl_enable_fast_reorg-new-in-v630) feature becomes generally available (GA), which improves the performance of adding indexes by about 10 times compared with v6.1.0.\\\\n- The TiDB global memory control becomes GA, and you can control the memory consumption threshold via [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-new-in-v640).\\\\n- The high-performance and globally monotonic [`AUTO_INCREMENT`](/auto-increment.md#mysql-compatibility-mode) column attribute becomes GA, which is compatible with MySQL.\\\\n- [`FLASHBACK CLUSTER TO TIMESTAMP`](/sql-statements/sql-statement-flashback-cluster.md) is now compatible with TiCDC and PITR and becomes GA.\\\\n- Enhance TiDB optimizer by making the more accurate [Cost Model version 2](/cost-model.md#cost-model-version-2) generally available and supporting expressions connected by `AND` for [INDEX MERGE](/explain-index-merge.md).\\\\n- Support pushing down the `JSON_EXTRACT()` function to TiFlash.\\\\n- Support [password management](/password-management.md) policies that meet password compliance auditing requirements.\\\\n- TiDB Lightning and Dumpling support [importing](/tidb-lightning/tidb-lightning-data-source.md) and [exporting](/dumpling-overview.md#improve-export-efficiency-through-concurrency) compressed SQL and CSV files.\\\\n- TiDB Data Migration (DM) [continuous data validation](/dm/dm-continuous-data-validation.md) becomes GA.\\\\n- TiDB Backup & Restore supports snapshot checkpoint backup, improves the recovery performance of [PITR](/br/br-pitr-guide.md#run-pitr) by 50%, and reduces the RPO in common scenarios to as short as 5 minutes.\\\\n- Improve the TiCDC throughput of [replicating data to Kafka](/replicate-data-to-kafka.md) from 4000 rows/s to 35000 rows/s, and reduce the replication latency to 2s.\\\\n- Provide row-level [Time to live (TTL)](/time-to-live.md) to manage data lifecycle (experimental).\\\\n- TiCDC supports [replicating changed logs to object storage](/ticdc/ticdc-sink-to-cloud-storage.md) such as Amazon S3, Azure Blob Storage, and NFS (experimental).\\\", \\\"## TiDB\\\\n\\\\n- SQL Optimizer\\\\n    - Optimize the selection range of `Index Join` to improve the execution performance\\\\n    - Optimize correlated subquery, push down `Filter`, and extend the index range, to improve the efficiency of some queries by orders of magnitude\\\\n    - Support `Index Hint` and `Join Hint` in the `UPDATE` and `DELETE` statements\\\\n    - Validate Hint `TIDM_SMJ` when no available index exists\\\\n    - Support pushdown of the `ABS`, `CEIL`, `FLOOR`, `IS TRUE`, and `IS FALSE` functions\\\\n    - Handle the `IF` and `IFNULL` functions especially in the constant folding process\\\\n- SQL Execution Engine\\\\n    - Implement parallel `Hash Aggregate` operators and improve the computing performance of `Hash Aggregate` by 350% in some scenarios\\\\n    - Implement parallel `Project` operators and improve the performance by 74% in some scenarios\\\\n    - Read the data of the `Inner` table and `Outer` table of `Hash Join` concurrently to improve the execution performance\\\\n    - Fix incorrect results of `INSERT \\\\u2026 ON DUPLICATE KEY UPDATE \\\\u2026` in some scenarios\\\\n    - Fix incorrect results of the `CONCAT_WS`, `FLOOR`, `CEIL`, and `DIV` built-in functions\\\\n- Server\\\\n    - Add the HTTP API to scatter the distribution of table Regions in the TiKV cluster\\\\n    - Add the `auto_analyze_ratio` system variable to control the threshold value of automatic `Analyze`\\\\n    - Add the HTTP API to control whether to open the general log\\\\n    - Add the HTTP API to modify the log level online\\\\n    - Add the user information in the general log and the slow query log\\\\n    - Support the server side cursor\\\\n- Compatibility\\\\n    - Support more MySQL syntax\\\\n    - Make the `bit` aggregate function support the `ALL` parameter\\\\n    - Support the `SHOW PRIVILEGES` statement\\\\n- DML\\\\n    - Decrease the memory usage of the `INSERT INTO SELECT` statement\\\\n    - Fix the performance issue of `PlanCache`\\\\n    - Add the `tidb_retry_limit` system variable to control the automatic retry times of transactions\\\\n    - Add the `tidb_disable_txn_auto_retry` system variable to control whether the transaction tries automatically\\\\n    - Fix the accuracy issue of the written data of the `time` type\\\\n    - Support the queue of locally conflicted transactions to optimize the conflicted transaction performance\\\\n    - Fix `Affected Rows` of the `UPDATE` statement\\\\n    - Optimize the statement performance of `insert ignore on duplicate key update`\\\\n- DDL\\\\n    - Optimize the execution speed of the `CreateTable` statement\\\\n    - Optimize the execution speed of `ADD INDEX` and improve it greatly in some scenarios\\\\n    - Fix the issue that the number of added columns by `Alter table add column` exceeds the limit of the number of table columns\\\\n    - Fix the issue that DDL job retries lead to an increasing pressure on TiKV in abnormal conditions\\\\n    - Fix the issue that TiDB continuously reloads the schema information in abnormal conditions\\\\n    - Do not output the `FOREIGN KEY` related information in the result of `SHOW CREATE TABLE`\\\\n    - Support the `select tidb_is_ddl_owner()` statement to facilitate judging whether TiDB is `DDL Owner`\\\\n    - Fix the issue that the index is deleted in the `Year` type in some scenarios\\\\n    - Fix the renaming table issue in the concurrent execution scenario\\\\n    - Support the `AlterTableForce` syntax\\\\n    - Support the `AlterTableRenameIndex` syntax with `FromKey` and `ToKey`\\\\n    - Add the table name and database name in the output information of `admin show ddl jobs`\\\"]}\", \"tool_call_id\": \"call_3LvLEfjdpcB1uV2ftxRI1CR0\"}\n",
      "\n",
      "\n",
      "call function VectorChunks {'query': 'TiDB v7.0.0 release notes performance improvements'}\n",
      "data: {\"role\": \"tool\", \"content\": \"{\\\"chunks_data\\\": [\\\"---\\\\ntitle: TiDB 7.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 7.5.0.\\\\n---\\\\n\\\\n# TiDB 7.5.0 Release Notes\\\\n\\\\nRelease date: December 1, 2023\\\\n\\\\nTiDB version: 7.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v7.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v7.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 7.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with the previous LTS 7.1.0, 7.5.0 includes new features, improvements, and bug fixes released in [7.2.0-DMR](/releases/release-7.2.0.md), [7.3.0-DMR](/releases/release-7.3.0.md), and [7.4.0-DMR](/releases/release-7.4.0.md). When you upgrade from 7.1.x to 7.5.0, you can download the [TiDB Release Notes PDF](https://download.pingcap.org/tidb-v7.2-to-v7.5-en-release-notes.pdf) to view all release notes between the two LTS versions. The following table lists some highlights from 7.2.0 to 7.5.0:\\\\n\\\\n<table>\\\\n<thead>\\\\n  <tr>\\\\n    <th>Category</th>\\\\n    <th>Feature</th>\\\\n    <th>Description</th>\\\\n  </tr>\\\\n</thead>\\\\n<tbody>\\\\n  <tr>\\\\n    <td>Scalability and Performance</td>\\\\n    <td>Support running multiple <code>ADD INDEX</code> statements in parallel</td>\\\\n    <td>This feature allows for concurrent jobs to add multiple indexes for a single table. Previously, it would take the time of X plus the time of Y to execute two <code>ADD INDEX</code> statements simultaneously (X and Y). With this feature, adding two indexes X and Y in one SQL can be concurrently executed, and the total execution time of DDL is significantly reduced. Especially in scenarios with wide tables, internal test data shows that performance can be improved by up to 94%.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"3\\\\\\\">Reliability and Availability</td>\\\\n    <td>Optimize <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-global-sort\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Global Sort</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>TiDB v7.1.0 introduced the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a>. For tasks that take advantage of this framework, v7.4 introduces global sorting to eliminate the unnecessary I/O, CPU, and memory spikes caused by temporarily out-of-order data during data re-organization tasks. The global sorting takes advantage of external shared object storage (Amazon S3 in this first iteration) to store intermediary files during the job, adding flexibility and cost savings. Operations like <code>ADD INDEX</code> and <code>IMPORT INTO</code> will be faster, more resilient, more stable, more flexible, and cost less to run.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-background-tasks\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource control for background tasks</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>In v7.1.0, the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> feature was introduced to mitigate resource and storage access interference between workloads. TiDB v7.4.0 applied this control to the priority of background tasks as well. In v7.4.0, Resource Control now identifies and manages the priority of background task execution, such as auto-analyze, Backup & Restore, bulk load with TiDB Lightning, and online DDL. In future releases, this control will eventually apply to all background tasks.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Resource control for <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-queries-that-consume-more-resources-than-expected-runaway-queries\\\\\\\"> managing runaway queries</a> (experimental, introduced in v7.2.0)</td>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> is a framework for resource-isolating workloads by Resource Groups, but it makes no calls on how individual queries affect work inside of each group. TiDB v7.2.0 introduces \\\\\\\"runaway queries control\\\\\\\" to let you control how TiDB identifies and treats these queries per Resource Group. Depending on needs, long running queries might be terminated or throttled, and the queries can be identified by exact SQL text, SQL digests or their plan digests, for better generalization. In v7.3.0, TiDB enables you to proactively watch for known bad queries, similar to a SQL blocklist at the database level.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>SQL</td>\\\\n    <td>MySQL 8.0 compatibility (introduced in v7.4.0)</td>\\\\n    <td>In MySQL 8.0, the default character set is utf8mb4, and the default collation of utf8mb4 is <code>utf8mb4_0900_ai_ci</code>. TiDB v7.4.0 adding support for this enhances compatibility with MySQL 8.0 so that migrations and replications from MySQL 8.0 databases with the default collation are now much smoother.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"4\\\\\\\">DB Operations and Observability</td>\\\\n    <td>TiDB Lightning's physical import mode integrated into TiDB with <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/sql-statement-import-into\\\\\\\"><code>IMPORT INTO</code></a> (GA)</td>\\\\n    <td>Before v7.2.0, to import data based on the file system, you needed to install <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-lightning-overview\\\\\\\">TiDB Lightning</a> and used its physical import mode. Now, the same capability is integrated into the <code>IMPORT INTO</code> statement so you can use this statement to quickly import data without installing any additional tool. This statement also supports the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a> for parallel import, which improves import efficiency during large-scale imports.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Specify <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/system-variables#tidb_service_scope-new-in-v740\\\\\\\" target=\\\\\\\"_blank\\\\\\\">the respective TiDB nodes</a> to execute the <code>ADD INDEX</code> and <code>IMPORT INTO</code> SQL statements (GA)</td>\\\\n    <td>You have the flexibility to specify whether to execute <code>ADD INDEX</code> or <code>IMPORT INTO</code> SQL statements on some of the existing TiDB nodes or newly added TiDB nodes. This approach enables resource isolation from the rest of the TiDB nodes, preventing any impact on business operations while ensuring optimal performance for executing the preceding SQL statements. In v7.5.0, this feature becomes generally available (GA).</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>DDL supports <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/ddl-introduction#ddl-related-commands\\\\\\\">pause and resume operations</a> (GA)</td>\\\\n    <td>Adding indexes can be big resource consumers and can affect online traffic. Even when throttled in a Resource Group or isolated to labeled nodes, there may still be a need to suspend these jobs in emergencies. As of v7.2.0, TiDB now natively supports suspending any number of these background jobs at once, freeing up needed resources while avoiding having to cancel and restart the jobs.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>TiDB Dashboard supports heap profiling for TiKV<a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/dashboard-profiling\\\\\\\" target=\\\\\\\"_blank\\\\\\\"></a></td>\\\\n    <td>Previously, addressing TiKV OOM or high memory usage issues typically required manual execution of <code>jeprof</code> to generate a heap profile in the instance environment. Starting from v7.5.0, TiKV enables remote processing of heap profiles. You can now directly access the flame graph and call graph of heap profile. This feature provides the same simple and easy-to-use experience as Go heap profiling.</td>\\\\n  </tr>\\\\n</tbody>\\\\n</table>\\\", \\\"---\\\\ntitle: TiDB 7.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 7.5.0.\\\\n---\\\\n\\\\n# TiDB 7.5.0 Release Notes\\\\n\\\\n<EmailSubscriptionWrapper />\\\\n\\\\nRelease date: December 1, 2023\\\\n\\\\nTiDB version: 7.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v7.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v7.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 7.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with the previous LTS 7.1.0, 7.5.0 includes new features, improvements, and bug fixes released in [7.2.0-DMR](/releases/release-7.2.0.md), [7.3.0-DMR](/releases/release-7.3.0.md), and [7.4.0-DMR](/releases/release-7.4.0.md). When you upgrade from 7.1.x to 7.5.0, you can download the [TiDB Release Notes PDF](https://download.pingcap.org/tidb-v7.2-to-v7.5-en-release-notes.pdf) to view all release notes between the two LTS versions. The following table lists some highlights from 7.2.0 to 7.5.0:\\\\n\\\\n<table>\\\\n<thead>\\\\n  <tr>\\\\n    <th>Category</th>\\\\n    <th>Feature</th>\\\\n    <th>Description</th>\\\\n  </tr>\\\\n</thead>\\\\n<tbody>\\\\n  <tr>\\\\n    <td>Scalability and Performance</td>\\\\n    <td>Support running multiple <code>ADD INDEX</code> statements in parallel</td>\\\\n    <td>This feature allows for concurrent jobs to add multiple indexes for a single table. Previously, it would take the time of X plus the time of Y to execute two <code>ADD INDEX</code> statements simultaneously (X and Y). With this feature, adding two indexes X and Y in one SQL can be concurrently executed, and the total execution time of DDL is significantly reduced. Especially in scenarios with wide tables, internal test data shows that performance can be improved by up to 94%.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"3\\\\\\\">Reliability and Availability</td>\\\\n    <td>Optimize <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-global-sort\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Global Sort</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>TiDB v7.1.0 introduced the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a>. For tasks that take advantage of this framework, v7.4 introduces global sorting to eliminate the unnecessary I/O, CPU, and memory spikes caused by temporarily out-of-order data during data re-organization tasks. The global sorting takes advantage of external shared object storage (Amazon S3 in this first iteration) to store intermediary files during the job, adding flexibility and cost savings. Operations like <code>ADD INDEX</code> and <code>IMPORT INTO</code> will be faster, more resilient, more stable, more flexible, and cost less to run.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-background-tasks\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource control for background tasks</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>In v7.1.0, the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> feature was introduced to mitigate resource and storage access interference between workloads. TiDB v7.4.0 applied this control to the priority of background tasks as well. In v7.4.0, Resource Control now identifies and manages the priority of background task execution, such as auto-analyze, Backup & Restore, bulk load with TiDB Lightning, and online DDL. In future releases, this control will eventually apply to all background tasks.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Resource control for <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-queries-that-consume-more-resources-than-expected-runaway-queries\\\\\\\"> managing runaway queries</a> (experimental, introduced in v7.2.0)</td>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> is a framework for resource-isolating workloads by Resource Groups, but it makes no calls on how individual queries affect work inside of each group. TiDB v7.2.0 introduces \\\\\\\"runaway queries control\\\\\\\" to let you control how TiDB identifies and treats these queries per Resource Group. Depending on needs, long running queries might be terminated or throttled, and the queries can be identified by exact SQL text, SQL digests or their plan digests, for better generalization. In v7.3.0, TiDB enables you to proactively watch for known bad queries, similar to a SQL blocklist at the database level.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>SQL</td>\\\\n    <td>MySQL 8.0 compatibility (introduced in v7.4.0)</td>\\\\n    <td>In MySQL 8.0, the default character set is utf8mb4, and the default collation of utf8mb4 is <code>utf8mb4_0900_ai_ci</code>. TiDB v7.4.0 adding support for this enhances compatibility with MySQL 8.0 so that migrations and replications from MySQL 8.0 databases with the default collation are now much smoother.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"4\\\\\\\">DB Operations and Observability</td>\\\\n    <td>TiDB Lightning's physical import mode integrated into TiDB with <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/sql-statement-import-into\\\\\\\"><code>IMPORT INTO</code></a> (GA)</td>\\\\n    <td>Before v7.2.0, to import data based on the file system, you needed to install <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-lightning-overview\\\\\\\">TiDB Lightning</a> and used its physical import mode. Now, the same capability is integrated into the <code>IMPORT INTO</code> statement so you can use this statement to quickly import data without installing any additional tool. This statement also supports the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a> for parallel import, which improves import efficiency during large-scale imports.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Specify <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/system-variables#tidb_service_scope-new-in-v740\\\\\\\" target=\\\\\\\"_blank\\\\\\\">the respective TiDB nodes</a> to execute the <code>ADD INDEX</code> and <code>IMPORT INTO</code> SQL statements (GA)</td>\\\\n    <td>You have the flexibility to specify whether to execute <code>ADD INDEX</code> or <code>IMPORT INTO</code> SQL statements on some of the existing TiDB nodes or newly added TiDB nodes. This approach enables resource isolation from the rest of the TiDB nodes, preventing any impact on business operations while ensuring optimal performance for executing the preceding SQL statements. In v7.5.0, this feature becomes generally available (GA).</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>DDL supports <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/ddl-introduction#ddl-related-commands\\\\\\\">pause and resume operations</a> (GA)</td>\\\\n    <td>Adding indexes can be big resource consumers and can affect online traffic. Even when throttled in a Resource Group or isolated to labeled nodes, there may still be a need to suspend these jobs in emergencies. As of v7.2.0, TiDB now natively supports suspending any number of these background jobs at once, freeing up needed resources while avoiding having to cancel and restart the jobs.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>TiDB Dashboard supports heap profiling for TiKV<a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/dashboard-profiling\\\\\\\" target=\\\\\\\"_blank\\\\\\\"></a></td>\\\\n    <td>Previously, addressing TiKV OOM or high memory usage issues typically required manual execution of <code>jeprof</code> to generate a heap profile in the instance environment. Starting from v7.5.0, TiKV enables remote processing of heap profiles. You can now directly access the flame graph and call graph of heap profile. This feature provides the same simple and easy-to-use experience as Go heap profiling.</td>\\\\n  </tr>\\\\n</tbody>\\\\n</table>\\\", \\\"## TiDB\\\\n\\\\n- SQL Optimizer\\\\n    - Use more compact data structure to reduce the memory usage of statistics information\\\\n    - Speed up loading statistics information when starting a tidb-server process\\\\n    - Support updating statistics information dynamically [experimental]\\\\n    - Optimize the cost model to provide more accurate query cost evaluation\\\\n    - Use `Count-Min Sketch` to estimate the cost of point queries more accurately\\\\n    - Support analyzing more complex conditions to make full use of indexes\\\\n    - Support manually specifying the `Join` order using the `STRAIGHT_JOIN` syntax\\\\n    - Use the Stream Aggregation operator when the `GROUP BY` clause is empty to improve the performance\\\\n    - Support using indexes for the `MAX/MIN` function\\\\n    - Optimize the processing algorithms for correlated subqueries to support decorrelating more types of correlated subqueries and transform them to `Left Outer Join`\\\\n    - Extend `IndexLookupJoin` to be used in matching the index prefix\\\\n- SQL Execution Engine\\\\n    - Refactor all operators using the Chunk architecture, improve the execution performance of analytical queries, and reduce memory usage. There is a significant improvement in the TPC-H benchmark result.\\\\n    - Support the Streaming Aggregation operators pushdown\\\\n    - Optimize the `Insert Into Ignore` statement to improve the performance by over 10 times\\\\n    - Optimize the `Insert On Duplicate Key Update` statement to improve the performance by over 10 times\\\\n    - Optimize `Load Data` to improve the performance by over 10 times\\\\n    - Push down more data types and functions to TiKV\\\\n    - Support computing the memory usage of physical operators, and specifying the processing behavior in the configuration file and system variables when the memory usage exceeds the threshold\\\\n    - Support limiting the memory usage by a single SQL statement to reduce the risk of OOM\\\\n    - Support using implicit RowID in CRUD operations\\\\n    - Improve the performance of point queries\\\\n- Server\\\\n    - Support the Proxy Protocol\\\\n    - Add more monitoring metrics and refine the log\\\\n    - Support validating the configuration files\\\\n    - Support obtaining the information of TiDB parameters through HTTP API\\\\n    - Resolve Lock in the Batch mode to speed up garbage collection\\\\n    - Support multi-threaded garbage collection\\\\n    - Support TLS\\\\n- Compatibility\\\\n    - Support more MySQL syntaxes\\\\n    - Support modifying the `lower_case_table_names` system variable in the configuration file to support the OGG data replication tool\\\\n    - Improve compatibility with the Navicat management tool\\\\n    - Support displaying the table creating time in `Information_Schema`\\\\n    - Fix the issue that the return types of some functions/expressions differ from MySQL\\\\n    - Improve compatibility with JDBC\\\\n    - Support more SQL Modes\\\\n- DDL\\\\n    - Optimize the `Add Index` operation to greatly improve the execution speed in some scenarios\\\\n    - Attach a lower priority to the `Add Index` operation to reduce the impact on online business\\\\n    - Output more detailed status information of the DDL jobs in `Admin Show DDL Jobs`\\\\n    - Support querying the original statements of currently running DDL jobs using `Admin Show DDL Job Queries JobID`\\\\n    - Support recovering the index data using `Admin Recover Index` for disaster recovery\\\\n    - Support modifying Table Options using the `Alter` statement\\\", \\\"## Improvements\\\\n\\\\n+ TiDB\\\\n\\\\n    - Allow setting placement rules and TiFlash replicas at the same time in one table [#37171](https://github.com/pingcap/tidb/issues/37171) @[lcwangchao](https://github.com/lcwangchao)\\\\n\\\\n+ TiKV\\\\n\\\\n    - Support configuring the `unreachable_backoff` item to avoid Raftstore broadcasting too many messages after one peer becomes unreachable [#13054](https://github.com/tikv/tikv/issues/13054) @[5kbpers](https://github.com/5kbpers)\\\\n    - Support configuring the RocksDB write stall settings to a value smaller than the flow control threshold [#13467](https://github.com/tikv/tikv/issues/13467) @[tabokie](https://github.com/tabokie)\\\\n\\\\n+ Tools\\\\n\\\\n    + TiDB Lightning\\\\n\\\\n        - Add retryable errors during checksum to improve robustness [#37690](https://github.com/pingcap/tidb/issues/37690) @[D3Hunter](https://github.com/D3Hunter)\\\\n\\\\n    + TiCDC\\\\n\\\\n        - Enhance the performance of the region worker by handling resolved TS in a batch [#7078](https://github.com/pingcap/tiflow/issues/7078) @[sdojjy](https://github.com/sdojjy)\\\", \\\"## TiDB\\\\n\\\\n- SQL Optimizer\\\\n    - Use more compact data structure to reduce the memory usage of statistics information\\\\n    - Speed up loading statistics information when starting a tidb-server process\\\\n    - Support updating statistics information dynamically [experimental]\\\\n    - Optimize the cost model to provide more accurate query cost evaluation\\\\n    - Use `Count-Min Sketch` to estimate the cost of point queries more accurately\\\\n    - Support analyzing more complex conditions to make full use of indexes\\\\n    - Support manually specifying the `Join` order using the `STRAIGHT_JOIN` syntax\\\\n    - Use the Stream Aggregation operator when the `GROUP BY` clause is empty to improve the performance\\\\n    - Support using indexes for the `MAX/MIN` function\\\\n    - Optimize the processing algorithms for correlated subqueries to support decorrelating more types of correlated subqueries and transform them to `Left Outer Join`\\\\n    - Extend `IndexLookupJoin` to be used in matching the index prefix\\\\n- SQL Execution Engine\\\\n    - Refactor all operators using the Chunk architecture, improve the execution performance of analytical queries, and reduce memory usage. There is a significant improvement in the TPC-H benchmark result.\\\\n    - Support the Streaming Aggregation operators pushdown\\\\n    - Optimize the `Insert Into Ignore` statement to improve the performance by over 10 times\\\\n    - Optimize the `Insert On Duplicate Key Update` statement to improve the performance by over 10 times\\\\n    - Optimize `Load Data` to improve the performance by over 10 times\\\\n    - Push down more data types and functions to TiKV\\\\n    - Support computing the memory usage of physical operators, and specifying the processing behavior in the configuration file and system variables when the memory usage exceeds the threshold\\\\n    - Support limiting the memory usage by a single SQL statement to reduce the risk of OOM\\\\n    - Support using implicit RowID in CRUD operations\\\\n    - Improve the performance of point queries\\\\n- Server\\\\n    - Support the Proxy Protocol\\\\n    - Add more monitoring metrics and refine the log\\\\n    - Support validating the configuration files\\\\n    - Support obtaining the information of TiDB parameters through HTTP API\\\\n    - Resolve Lock in the Batch mode to speed up garbage collection\\\\n    - Support multi-threaded garbage collection\\\\n    - Support TLS\\\\n- Compatibility\\\\n    - Support more MySQL syntaxes\\\\n    - Support modifying the `lower_case_table_names` system variable in the configuration file to support the OGG data replication tool\\\\n    - Improve compatibility with the Navicat management tool\\\\n    - Support displaying the table creating time in `Information_Schema`\\\\n    - Fix the issue that the return types of some functions/expressions differ from MySQL\\\\n    - Improve compatibility with JDBC\\\\n    - Support more SQL Modes\\\\n- DDL\\\\n    - Optimize the `Add Index` operation to greatly improve the execution speed in some scenarios\\\\n    - Attach a lower priority to the `Add Index` operation to reduce the impact on online business\\\\n    - Output more detailed status information of the DDL jobs in `Admin Show DDL Jobs`\\\\n    - Support querying the original statements of currently running DDL jobs using `Admin Show DDL Job Queries JobID`\\\\n    - Support recovering the index data using `Admin Recover Index` for disaster recovery\\\\n    - Support modifying Table Options using the `Alter` statement\\\"]}\", \"tool_call_id\": \"call_DeiHtFoMG5kpIPx4cDoVdR3r\"}\n",
      "\n",
      "\n",
      "call function VectorChunks {'query': 'TiDB v7.1.1 release notes performance improvements'}\n",
      "data: {\"role\": \"tool\", \"content\": \"{\\\"chunks_data\\\": [\\\"---\\\\ntitle: TiDB 7.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 7.5.0.\\\\n---\\\\n\\\\n# TiDB 7.5.0 Release Notes\\\\n\\\\nRelease date: December 1, 2023\\\\n\\\\nTiDB version: 7.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v7.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v7.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 7.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with the previous LTS 7.1.0, 7.5.0 includes new features, improvements, and bug fixes released in [7.2.0-DMR](/releases/release-7.2.0.md), [7.3.0-DMR](/releases/release-7.3.0.md), and [7.4.0-DMR](/releases/release-7.4.0.md). When you upgrade from 7.1.x to 7.5.0, you can download the [TiDB Release Notes PDF](https://download.pingcap.org/tidb-v7.2-to-v7.5-en-release-notes.pdf) to view all release notes between the two LTS versions. The following table lists some highlights from 7.2.0 to 7.5.0:\\\\n\\\\n<table>\\\\n<thead>\\\\n  <tr>\\\\n    <th>Category</th>\\\\n    <th>Feature</th>\\\\n    <th>Description</th>\\\\n  </tr>\\\\n</thead>\\\\n<tbody>\\\\n  <tr>\\\\n    <td>Scalability and Performance</td>\\\\n    <td>Support running multiple <code>ADD INDEX</code> statements in parallel</td>\\\\n    <td>This feature allows for concurrent jobs to add multiple indexes for a single table. Previously, it would take the time of X plus the time of Y to execute two <code>ADD INDEX</code> statements simultaneously (X and Y). With this feature, adding two indexes X and Y in one SQL can be concurrently executed, and the total execution time of DDL is significantly reduced. Especially in scenarios with wide tables, internal test data shows that performance can be improved by up to 94%.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"3\\\\\\\">Reliability and Availability</td>\\\\n    <td>Optimize <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-global-sort\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Global Sort</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>TiDB v7.1.0 introduced the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a>. For tasks that take advantage of this framework, v7.4 introduces global sorting to eliminate the unnecessary I/O, CPU, and memory spikes caused by temporarily out-of-order data during data re-organization tasks. The global sorting takes advantage of external shared object storage (Amazon S3 in this first iteration) to store intermediary files during the job, adding flexibility and cost savings. Operations like <code>ADD INDEX</code> and <code>IMPORT INTO</code> will be faster, more resilient, more stable, more flexible, and cost less to run.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-background-tasks\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource control for background tasks</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>In v7.1.0, the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> feature was introduced to mitigate resource and storage access interference between workloads. TiDB v7.4.0 applied this control to the priority of background tasks as well. In v7.4.0, Resource Control now identifies and manages the priority of background task execution, such as auto-analyze, Backup & Restore, bulk load with TiDB Lightning, and online DDL. In future releases, this control will eventually apply to all background tasks.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Resource control for <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-queries-that-consume-more-resources-than-expected-runaway-queries\\\\\\\"> managing runaway queries</a> (experimental, introduced in v7.2.0)</td>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> is a framework for resource-isolating workloads by Resource Groups, but it makes no calls on how individual queries affect work inside of each group. TiDB v7.2.0 introduces \\\\\\\"runaway queries control\\\\\\\" to let you control how TiDB identifies and treats these queries per Resource Group. Depending on needs, long running queries might be terminated or throttled, and the queries can be identified by exact SQL text, SQL digests or their plan digests, for better generalization. In v7.3.0, TiDB enables you to proactively watch for known bad queries, similar to a SQL blocklist at the database level.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>SQL</td>\\\\n    <td>MySQL 8.0 compatibility (introduced in v7.4.0)</td>\\\\n    <td>In MySQL 8.0, the default character set is utf8mb4, and the default collation of utf8mb4 is <code>utf8mb4_0900_ai_ci</code>. TiDB v7.4.0 adding support for this enhances compatibility with MySQL 8.0 so that migrations and replications from MySQL 8.0 databases with the default collation are now much smoother.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"4\\\\\\\">DB Operations and Observability</td>\\\\n    <td>TiDB Lightning's physical import mode integrated into TiDB with <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/sql-statement-import-into\\\\\\\"><code>IMPORT INTO</code></a> (GA)</td>\\\\n    <td>Before v7.2.0, to import data based on the file system, you needed to install <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-lightning-overview\\\\\\\">TiDB Lightning</a> and used its physical import mode. Now, the same capability is integrated into the <code>IMPORT INTO</code> statement so you can use this statement to quickly import data without installing any additional tool. This statement also supports the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a> for parallel import, which improves import efficiency during large-scale imports.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Specify <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/system-variables#tidb_service_scope-new-in-v740\\\\\\\" target=\\\\\\\"_blank\\\\\\\">the respective TiDB nodes</a> to execute the <code>ADD INDEX</code> and <code>IMPORT INTO</code> SQL statements (GA)</td>\\\\n    <td>You have the flexibility to specify whether to execute <code>ADD INDEX</code> or <code>IMPORT INTO</code> SQL statements on some of the existing TiDB nodes or newly added TiDB nodes. This approach enables resource isolation from the rest of the TiDB nodes, preventing any impact on business operations while ensuring optimal performance for executing the preceding SQL statements. In v7.5.0, this feature becomes generally available (GA).</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>DDL supports <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/ddl-introduction#ddl-related-commands\\\\\\\">pause and resume operations</a> (GA)</td>\\\\n    <td>Adding indexes can be big resource consumers and can affect online traffic. Even when throttled in a Resource Group or isolated to labeled nodes, there may still be a need to suspend these jobs in emergencies. As of v7.2.0, TiDB now natively supports suspending any number of these background jobs at once, freeing up needed resources while avoiding having to cancel and restart the jobs.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>TiDB Dashboard supports heap profiling for TiKV<a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/dashboard-profiling\\\\\\\" target=\\\\\\\"_blank\\\\\\\"></a></td>\\\\n    <td>Previously, addressing TiKV OOM or high memory usage issues typically required manual execution of <code>jeprof</code> to generate a heap profile in the instance environment. Starting from v7.5.0, TiKV enables remote processing of heap profiles. You can now directly access the flame graph and call graph of heap profile. This feature provides the same simple and easy-to-use experience as Go heap profiling.</td>\\\\n  </tr>\\\\n</tbody>\\\\n</table>\\\", \\\"## TiDB\\\\n\\\\n- SQL Optimizer\\\\n    - Use more compact data structure to reduce the memory usage of statistics information\\\\n    - Speed up loading statistics information when starting a tidb-server process\\\\n    - Support updating statistics information dynamically [experimental]\\\\n    - Optimize the cost model to provide more accurate query cost evaluation\\\\n    - Use `Count-Min Sketch` to estimate the cost of point queries more accurately\\\\n    - Support analyzing more complex conditions to make full use of indexes\\\\n    - Support manually specifying the `Join` order using the `STRAIGHT_JOIN` syntax\\\\n    - Use the Stream Aggregation operator when the `GROUP BY` clause is empty to improve the performance\\\\n    - Support using indexes for the `MAX/MIN` function\\\\n    - Optimize the processing algorithms for correlated subqueries to support decorrelating more types of correlated subqueries and transform them to `Left Outer Join`\\\\n    - Extend `IndexLookupJoin` to be used in matching the index prefix\\\\n- SQL Execution Engine\\\\n    - Refactor all operators using the Chunk architecture, improve the execution performance of analytical queries, and reduce memory usage. There is a significant improvement in the TPC-H benchmark result.\\\\n    - Support the Streaming Aggregation operators pushdown\\\\n    - Optimize the `Insert Into Ignore` statement to improve the performance by over 10 times\\\\n    - Optimize the `Insert On Duplicate Key Update` statement to improve the performance by over 10 times\\\\n    - Optimize `Load Data` to improve the performance by over 10 times\\\\n    - Push down more data types and functions to TiKV\\\\n    - Support computing the memory usage of physical operators, and specifying the processing behavior in the configuration file and system variables when the memory usage exceeds the threshold\\\\n    - Support limiting the memory usage by a single SQL statement to reduce the risk of OOM\\\\n    - Support using implicit RowID in CRUD operations\\\\n    - Improve the performance of point queries\\\\n- Server\\\\n    - Support the Proxy Protocol\\\\n    - Add more monitoring metrics and refine the log\\\\n    - Support validating the configuration files\\\\n    - Support obtaining the information of TiDB parameters through HTTP API\\\\n    - Resolve Lock in the Batch mode to speed up garbage collection\\\\n    - Support multi-threaded garbage collection\\\\n    - Support TLS\\\\n- Compatibility\\\\n    - Support more MySQL syntaxes\\\\n    - Support modifying the `lower_case_table_names` system variable in the configuration file to support the OGG data replication tool\\\\n    - Improve compatibility with the Navicat management tool\\\\n    - Support displaying the table creating time in `Information_Schema`\\\\n    - Fix the issue that the return types of some functions/expressions differ from MySQL\\\\n    - Improve compatibility with JDBC\\\\n    - Support more SQL Modes\\\\n- DDL\\\\n    - Optimize the `Add Index` operation to greatly improve the execution speed in some scenarios\\\\n    - Attach a lower priority to the `Add Index` operation to reduce the impact on online business\\\\n    - Output more detailed status information of the DDL jobs in `Admin Show DDL Jobs`\\\\n    - Support querying the original statements of currently running DDL jobs using `Admin Show DDL Job Queries JobID`\\\\n    - Support recovering the index data using `Admin Recover Index` for disaster recovery\\\\n    - Support modifying Table Options using the `Alter` statement\\\", \\\"## TiDB\\\\n\\\\n- SQL Optimizer\\\\n    - Use more compact data structure to reduce the memory usage of statistics information\\\\n    - Speed up loading statistics information when starting a tidb-server process\\\\n    - Support updating statistics information dynamically [experimental]\\\\n    - Optimize the cost model to provide more accurate query cost evaluation\\\\n    - Use `Count-Min Sketch` to estimate the cost of point queries more accurately\\\\n    - Support analyzing more complex conditions to make full use of indexes\\\\n    - Support manually specifying the `Join` order using the `STRAIGHT_JOIN` syntax\\\\n    - Use the Stream Aggregation operator when the `GROUP BY` clause is empty to improve the performance\\\\n    - Support using indexes for the `MAX/MIN` function\\\\n    - Optimize the processing algorithms for correlated subqueries to support decorrelating more types of correlated subqueries and transform them to `Left Outer Join`\\\\n    - Extend `IndexLookupJoin` to be used in matching the index prefix\\\\n- SQL Execution Engine\\\\n    - Refactor all operators using the Chunk architecture, improve the execution performance of analytical queries, and reduce memory usage. There is a significant improvement in the TPC-H benchmark result.\\\\n    - Support the Streaming Aggregation operators pushdown\\\\n    - Optimize the `Insert Into Ignore` statement to improve the performance by over 10 times\\\\n    - Optimize the `Insert On Duplicate Key Update` statement to improve the performance by over 10 times\\\\n    - Optimize `Load Data` to improve the performance by over 10 times\\\\n    - Push down more data types and functions to TiKV\\\\n    - Support computing the memory usage of physical operators, and specifying the processing behavior in the configuration file and system variables when the memory usage exceeds the threshold\\\\n    - Support limiting the memory usage by a single SQL statement to reduce the risk of OOM\\\\n    - Support using implicit RowID in CRUD operations\\\\n    - Improve the performance of point queries\\\\n- Server\\\\n    - Support the Proxy Protocol\\\\n    - Add more monitoring metrics and refine the log\\\\n    - Support validating the configuration files\\\\n    - Support obtaining the information of TiDB parameters through HTTP API\\\\n    - Resolve Lock in the Batch mode to speed up garbage collection\\\\n    - Support multi-threaded garbage collection\\\\n    - Support TLS\\\\n- Compatibility\\\\n    - Support more MySQL syntaxes\\\\n    - Support modifying the `lower_case_table_names` system variable in the configuration file to support the OGG data replication tool\\\\n    - Improve compatibility with the Navicat management tool\\\\n    - Support displaying the table creating time in `Information_Schema`\\\\n    - Fix the issue that the return types of some functions/expressions differ from MySQL\\\\n    - Improve compatibility with JDBC\\\\n    - Support more SQL Modes\\\\n- DDL\\\\n    - Optimize the `Add Index` operation to greatly improve the execution speed in some scenarios\\\\n    - Attach a lower priority to the `Add Index` operation to reduce the impact on online business\\\\n    - Output more detailed status information of the DDL jobs in `Admin Show DDL Jobs`\\\\n    - Support querying the original statements of currently running DDL jobs using `Admin Show DDL Job Queries JobID`\\\\n    - Support recovering the index data using `Admin Recover Index` for disaster recovery\\\\n    - Support modifying Table Options using the `Alter` statement\\\", \\\"## Improvements\\\\n\\\\n+ TiDB\\\\n\\\\n    - Allow setting placement rules and TiFlash replicas at the same time in one table [#37171](https://github.com/pingcap/tidb/issues/37171) @[lcwangchao](https://github.com/lcwangchao)\\\\n\\\\n+ TiKV\\\\n\\\\n    - Support configuring the `unreachable_backoff` item to avoid Raftstore broadcasting too many messages after one peer becomes unreachable [#13054](https://github.com/tikv/tikv/issues/13054) @[5kbpers](https://github.com/5kbpers)\\\\n    - Support configuring the RocksDB write stall settings to a value smaller than the flow control threshold [#13467](https://github.com/tikv/tikv/issues/13467) @[tabokie](https://github.com/tabokie)\\\\n\\\\n+ Tools\\\\n\\\\n    + TiDB Lightning\\\\n\\\\n        - Add retryable errors during checksum to improve robustness [#37690](https://github.com/pingcap/tidb/issues/37690) @[D3Hunter](https://github.com/D3Hunter)\\\\n\\\\n    + TiCDC\\\\n\\\\n        - Enhance the performance of the region worker by handling resolved TS in a batch [#7078](https://github.com/pingcap/tiflow/issues/7078) @[sdojjy](https://github.com/sdojjy)\\\", \\\"---\\\\ntitle: TiDB 7.5.0 Release Notes\\\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 7.5.0.\\\\n---\\\\n\\\\n# TiDB 7.5.0 Release Notes\\\\n\\\\n<EmailSubscriptionWrapper />\\\\n\\\\nRelease date: December 1, 2023\\\\n\\\\nTiDB version: 7.5.0\\\\n\\\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v7.5/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v7.5/production-deployment-using-tiup)\\\\n\\\\nTiDB 7.5.0 is a Long-Term Support Release (LTS).\\\\n\\\\nCompared with the previous LTS 7.1.0, 7.5.0 includes new features, improvements, and bug fixes released in [7.2.0-DMR](/releases/release-7.2.0.md), [7.3.0-DMR](/releases/release-7.3.0.md), and [7.4.0-DMR](/releases/release-7.4.0.md). When you upgrade from 7.1.x to 7.5.0, you can download the [TiDB Release Notes PDF](https://download.pingcap.org/tidb-v7.2-to-v7.5-en-release-notes.pdf) to view all release notes between the two LTS versions. The following table lists some highlights from 7.2.0 to 7.5.0:\\\\n\\\\n<table>\\\\n<thead>\\\\n  <tr>\\\\n    <th>Category</th>\\\\n    <th>Feature</th>\\\\n    <th>Description</th>\\\\n  </tr>\\\\n</thead>\\\\n<tbody>\\\\n  <tr>\\\\n    <td>Scalability and Performance</td>\\\\n    <td>Support running multiple <code>ADD INDEX</code> statements in parallel</td>\\\\n    <td>This feature allows for concurrent jobs to add multiple indexes for a single table. Previously, it would take the time of X plus the time of Y to execute two <code>ADD INDEX</code> statements simultaneously (X and Y). With this feature, adding two indexes X and Y in one SQL can be concurrently executed, and the total execution time of DDL is significantly reduced. Especially in scenarios with wide tables, internal test data shows that performance can be improved by up to 94%.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"3\\\\\\\">Reliability and Availability</td>\\\\n    <td>Optimize <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-global-sort\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Global Sort</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>TiDB v7.1.0 introduced the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a>. For tasks that take advantage of this framework, v7.4 introduces global sorting to eliminate the unnecessary I/O, CPU, and memory spikes caused by temporarily out-of-order data during data re-organization tasks. The global sorting takes advantage of external shared object storage (Amazon S3 in this first iteration) to store intermediary files during the job, adding flexibility and cost savings. Operations like <code>ADD INDEX</code> and <code>IMPORT INTO</code> will be faster, more resilient, more stable, more flexible, and cost less to run.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-background-tasks\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource control for background tasks</a> (experimental, introduced in v7.4.0)</td>\\\\n    <td>In v7.1.0, the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> feature was introduced to mitigate resource and storage access interference between workloads. TiDB v7.4.0 applied this control to the priority of background tasks as well. In v7.4.0, Resource Control now identifies and manages the priority of background task execution, such as auto-analyze, Backup & Restore, bulk load with TiDB Lightning, and online DDL. In future releases, this control will eventually apply to all background tasks.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Resource control for <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control#manage-queries-that-consume-more-resources-than-expected-runaway-queries\\\\\\\"> managing runaway queries</a> (experimental, introduced in v7.2.0)</td>\\\\n    <td><a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-resource-control\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Resource Control</a> is a framework for resource-isolating workloads by Resource Groups, but it makes no calls on how individual queries affect work inside of each group. TiDB v7.2.0 introduces \\\\\\\"runaway queries control\\\\\\\" to let you control how TiDB identifies and treats these queries per Resource Group. Depending on needs, long running queries might be terminated or throttled, and the queries can be identified by exact SQL text, SQL digests or their plan digests, for better generalization. In v7.3.0, TiDB enables you to proactively watch for known bad queries, similar to a SQL blocklist at the database level.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>SQL</td>\\\\n    <td>MySQL 8.0 compatibility (introduced in v7.4.0)</td>\\\\n    <td>In MySQL 8.0, the default character set is utf8mb4, and the default collation of utf8mb4 is <code>utf8mb4_0900_ai_ci</code>. TiDB v7.4.0 adding support for this enhances compatibility with MySQL 8.0 so that migrations and replications from MySQL 8.0 databases with the default collation are now much smoother.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td rowspan=\\\\\\\"4\\\\\\\">DB Operations and Observability</td>\\\\n    <td>TiDB Lightning's physical import mode integrated into TiDB with <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/sql-statement-import-into\\\\\\\"><code>IMPORT INTO</code></a> (GA)</td>\\\\n    <td>Before v7.2.0, to import data based on the file system, you needed to install <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-lightning-overview\\\\\\\">TiDB Lightning</a> and used its physical import mode. Now, the same capability is integrated into the <code>IMPORT INTO</code> statement so you can use this statement to quickly import data without installing any additional tool. This statement also supports the <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/tidb-distributed-execution-framework\\\\\\\" target=\\\\\\\"_blank\\\\\\\">Distributed eXecution Framework (DXF)</a> for parallel import, which improves import efficiency during large-scale imports.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>Specify <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/system-variables#tidb_service_scope-new-in-v740\\\\\\\" target=\\\\\\\"_blank\\\\\\\">the respective TiDB nodes</a> to execute the <code>ADD INDEX</code> and <code>IMPORT INTO</code> SQL statements (GA)</td>\\\\n    <td>You have the flexibility to specify whether to execute <code>ADD INDEX</code> or <code>IMPORT INTO</code> SQL statements on some of the existing TiDB nodes or newly added TiDB nodes. This approach enables resource isolation from the rest of the TiDB nodes, preventing any impact on business operations while ensuring optimal performance for executing the preceding SQL statements. In v7.5.0, this feature becomes generally available (GA).</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>DDL supports <a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/ddl-introduction#ddl-related-commands\\\\\\\">pause and resume operations</a> (GA)</td>\\\\n    <td>Adding indexes can be big resource consumers and can affect online traffic. Even when throttled in a Resource Group or isolated to labeled nodes, there may still be a need to suspend these jobs in emergencies. As of v7.2.0, TiDB now natively supports suspending any number of these background jobs at once, freeing up needed resources while avoiding having to cancel and restart the jobs.</td>\\\\n  </tr>\\\\n  <tr>\\\\n    <td>TiDB Dashboard supports heap profiling for TiKV<a href=\\\\\\\"https://docs.pingcap.com/tidb/v7.5/dashboard-profiling\\\\\\\" target=\\\\\\\"_blank\\\\\\\"></a></td>\\\\n    <td>Previously, addressing TiKV OOM or high memory usage issues typically required manual execution of <code>jeprof</code> to generate a heap profile in the instance environment. Starting from v7.5.0, TiKV enables remote processing of heap profiles. You can now directly access the flame graph and call graph of heap profile. This feature provides the same simple and easy-to-use experience as Go heap profiling.</td>\\\\n  </tr>\\\\n</tbody>\\\\n</table>\\\"]}\", \"tool_call_id\": \"call_3HfU3Wef0cPmM29fZDIBowhp\"}\n",
      "\n",
      "\n",
      "data: {\"role\": \"assistant\", \"content\": \"Here is a summary of the performance improvements of TiDB from version 6.5 to the newest version 7.1.1, based on the release notes:\\n\\n### Version 6.5.0\\n**Release Date:** December 29, 2022\\n1. **Index Acceleration**:\\n   - Improvement in the performance of adding indexes by about 10 times compared to v6.1.0.\\n2. **Global Memory Control**:\\n   - Enhances the TiDB global memory control to manage memory consumption thresholds.\\n3. **AUTO_INCREMENT**:\\n   - High-performance and globally monotonic `AUTO_INCREMENT` column becomes GA.\\n4. **Optimizer Enhancements**:\\n   - More accurate Cost Model version 2 becomes GA and support for expressions connected by `AND` for INDEX MERGE.\\n5. **Improved TiCDC Throughput**:\\n   - Throughput of replicating data to Kafka increased from 4000 rows/s to 35000 rows/s and reducing latency to 2s.\\n6. **Other Enhancements**:\\n   - Enhancements in TiDB Lightning, TiDB Data Migration, Backup & Restore performance, and other operations.\\n\\n### Version 7.0.0\\n**Key Highlights**:\\n1. **SQL Optimizer Enhancements**:\\n   - Enhanced selection range of `Index Join`, correlated subquery optimization, support for `Index Hint` and `Join Hint` in `UPDATE` and `DELETE` statements.\\n   - Improved performance in query efficiency through various optimizations.\\n2. **SQL Execution Engine**:\\n   - Implementation of parallel `Hash Aggregate` operators improving computing performance by 350% in some scenarios.\\n   - Parallel `Project` operators for improved performance by 74% in some scenarios.\\n3. **Server Enhancements**:\\n   - Added APIs for scatter distribution of table Regions in the TiKV cluster and for controlling log levels online.\\n4. **Compatibility Enhancements**:\\n   - Added support for more MySQL syntax and features.\\n\\n### Version 7.1.1\\n**Key Highlights**:\\n1. **Concurrent Index Addition**:\\n   - Added support for running multiple `ADD INDEX` statements in parallel, significantly reducing DDL execution time and improving performance in scenarios with wide tables by up to 94%.\\n2. **Global Sort Optimization**:\\n   - Introduced to optimize operations like `ADD INDEX` and `IMPORT INTO`, adding resilience and reducing costs.\\n3. **Resource Control**:\\n   - Enhanced management of background tasks and runaway queries, improving reliability and availability.\\n4. **MySQL 8.0 Compatibility**:\\n   - Adding support for default character set utf8mb4 and collation for smoother migrations and replications.\\n5. **DB Operations and Observability**:\\n   - Integration of TiDB Lightning's physical import mode into TiDB for improved data import efficiency.\\n   - Support for specifying respective TiDB nodes to execute heavy operations like `ADD INDEX` and `IMPORT INTO`, ensuring optimal performance without impacting business operations.\\n\\n### Summary of Performance Improvements\\nFrom TiDB version 6.5 to the newest version 7.1.1, key performance improvements include:\\n- **Significant reduction in the time required for adding indexes through parallel execution and index acceleration.**\\n- **Enhancements in SQL optimizer and execution engine for better query performance and resource management.**\\n- **Improvements in data migration and replication performance, notably with TiCDC throughput and latency optimization.**\\n- **New support features for better compatibility with MySQL and resource control to manage background tasks efficiently.**\\n\\nThese improvements collectively lead to a more powerful, efficient, and reliable TiDB system from version 6.5 to 7.1.1.\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for event in cs.chat(session, [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"could you summary the performance improvement of tidb from version 6.5 to newest version\"\n",
    "    }]):\n",
    "        print(f\"data: {json.dumps(event.payload)}\\n\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"data: {json.dumps({'event_type': 'ERROR', 'payload': str(e)})}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
